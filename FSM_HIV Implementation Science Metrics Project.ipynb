{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe77a3e-392d-40ff-b9fd-fdbe38e4feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################\n",
    "### Import Libraries and Dependencies ####\n",
    "##########################################\n",
    "\n",
    "from Bio import Entrez\n",
    "import pandas as pd\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import xlsxwriter\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import spacy\n",
    "import ast\n",
    "from geopy.geocoders import Nominatim\n",
    "import re\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from config import ENTREZ_EMAIL, ENTREZ_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c49372-f878-4fc0-a6b2-c35068f21790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Check if Spacy is working\n",
    "# # Try loading the model\n",
    "# try:\n",
    "#     nlp = spacy.load(\"en_core_web_sm\")\n",
    "#     print(\"‚úì spaCy is working!\")\n",
    "    \n",
    "#     # Test on sample text\n",
    "#     doc = nlp(\"Study conducted in Chicago, Illinois and New York.\")\n",
    "#     locations = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
    "#     print(f\"Found locations: {locations}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb4c8bf-5e5d-48ab-b917-b5bd2fd5271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################\n",
    "#### Functions for Sorting Data from Entrez PubMed API call ##############\n",
    "##########################################################################\n",
    "\n",
    "def fetch_pubmed_record(pubmed_id):\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=pubmed_id, rettype=\"xml\")\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    return record\n",
    "\n",
    "\n",
    "def get_journal_volume_issue(record):\n",
    "    try:      \n",
    "        journal_volume_issue = record['PubmedArticle'][0]['MedlineCitation']['Article']['Journal']['JournalIssue']\n",
    "        volume = journal_volume_issue.get('Volume', '')\n",
    "        issue = journal_volume_issue.get('Issue', '')\n",
    "        return volume, issue\n",
    "    except KeyError:\n",
    "        return None, None\n",
    "\n",
    "   \n",
    "\n",
    "def get_article_title_page(record):\n",
    "    try:      \n",
    "        article_title_page = record['PubmedArticle'][0]['MedlineCitation']['Article']\n",
    "        article_title= article_title_page.get('ArticleTitle', '')\n",
    "        page_start = article_title_page.get('Pagination', {}).get('MedlinePgn', '').split('-')[0]\n",
    "        page_end = article_title_page.get('Pagination', {}).get('MedlinePgn', '').split('-')[-1]\n",
    "        return article_title, page_start, page_end\n",
    "    except KeyError:\n",
    "        return None, None, None\n",
    "\n",
    "def get_journal_title(record):\n",
    "    try:\n",
    "        journal_title= record['PubmedArticle'][0]['MedlineCitation']['Article']['Journal']\n",
    "        #print(\"Journal Title: \", journal_title_pmid)\n",
    "        title = journal_title.get('Title', '')\n",
    "        return title\n",
    "    except KeyError:\n",
    "        return None, None, \n",
    "    \n",
    "def get_authors(record):\n",
    "    try:\n",
    "        authors = record['PubmedArticle'][0]['MedlineCitation']['Article']['AuthorList']\n",
    "        return ', '.join(author.get('LastName', '') + ' ' + author.get('Initials', '') for author in authors)\n",
    "    except KeyError:\n",
    "        return None\n",
    "    \n",
    "def get_publication_date_year(record):\n",
    "    try:\n",
    "        date = record['PubmedArticle'][0]['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']\n",
    "        return date.get('Year', '') if 'Year' in date else None\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def get_publication_date_month_year(record):\n",
    "    try:\n",
    "        date = record['PubmedArticle'][0]['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']\n",
    "        year = date.get('Year', '')\n",
    "        month = date.get('Month', '')\n",
    "        return f\"{month} {year}\" if month and year else None\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def get_publication_date_month_day_year(record):\n",
    "    try:\n",
    "        date = record['PubmedArticle'][0]['MedlineCitation']['Article']['Journal']['JournalIssue']['PubDate']\n",
    "        year = date.get('Year', '')\n",
    "        month = date.get('Month', '')\n",
    "        day = date.get('Day', '')\n",
    "\n",
    "        # Mapping of month abbreviations to numbers\n",
    "        month_mapping = {\n",
    "            'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',\n",
    "            'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'\n",
    "        }\n",
    "\n",
    "        # Replace the month abbreviation with the corresponding number\n",
    "        month_number = month_mapping.get(month, month)\n",
    "\n",
    "        return f\"{month_number}/{day}/{year}\" if month and day and year else None\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def get_abstract(record):\n",
    "    \"\"\"Extract abstract text from PubMed record\"\"\"\n",
    "    try:\n",
    "        abstract_texts = record['PubmedArticle'][0]['MedlineCitation']['Article'].get('Abstract', {}).get('AbstractText', [])\n",
    "        if isinstance(abstract_texts, list):\n",
    "            # Handle structured abstracts\n",
    "            abstract = ' '.join(str(text) for text in abstract_texts)\n",
    "        else:\n",
    "            abstract = str(abstract_texts)\n",
    "        return abstract if abstract else None\n",
    "    except (KeyError, IndexError):\n",
    "        return None\n",
    "        \n",
    "def get_pmid_pmcid_doi(record):\n",
    "    try:\n",
    "        pmid = next(\n",
    "            (id_ for id_ in record.get('PubmedArticle', [{}])[0].get('PubmedData', {}).get('ArticleIdList', []) if id_.attributes.get('IdType') == 'pubmed'),\n",
    "            None\n",
    "        )\n",
    "        #print(\"PMID: \", pmid)\n",
    "\n",
    "        pmcid = next(\n",
    "            (id_ for id_ in record.get('PubmedArticle', [{}])[0].get('PubmedData', {}).get('ArticleIdList', []) if id_.attributes.get('IdType') == 'pmc'),\n",
    "            None\n",
    "        )\n",
    "        #print(\"PMCID: \", pmcid)\n",
    "\n",
    "        doi = next(\n",
    "            (id_ for id_ in record.get('PubmedArticle', [{}])[0].get('PubmedData', {}).get('ArticleIdList', []) if id_.attributes.get('IdType') == 'doi'),\n",
    "            None\n",
    "        )\n",
    "        #print(\"DOI: \", doi)\n",
    "\n",
    "        return pmid, pmcid, doi\n",
    "    except (IndexError, KeyError):\n",
    "        return None, None, None\n",
    "\n",
    "\n",
    "\n",
    "def get_authors_with_affiliation_name_full(record, affiliations_to_check):\n",
    "    try:\n",
    "        authors = record['PubmedArticle'][0]['MedlineCitation']['Article']['AuthorList']\n",
    "        authors_with_affiliation_name_full = []\n",
    "              \n",
    "        for author in authors:\n",
    "            author_affiliations = author.get('AffiliationInfo', [])\n",
    "\n",
    "            if not author_affiliations:\n",
    "                # If there's no specific affiliation information, check the overall affiliation field\n",
    "                affiliations = [author.get('Affiliation', '').lower()]\n",
    "            else:\n",
    "                # If there's specific affiliation information, extract and check each affiliation\n",
    "                affiliations = [affiliation.get('Affiliation', '').lower() for affiliation in author_affiliations]\n",
    "\n",
    "                \n",
    "\n",
    "            # Check if any phrase in affiliations_to_check is a substring of affiliation\n",
    "            if any(phrase.lower() in affiliation for phrase in affiliations_to_check for affiliation in affiliations):\n",
    "                full_name = author.get('LastName', '') + ' ' + author.get('ForeName', '')\n",
    "                authors_with_affiliation_name_full.append(full_name)\n",
    "                              \n",
    "        return authors_with_affiliation_name_full if authors_with_affiliation_name_full else None\n",
    "    except KeyError:\n",
    "        return None\n",
    "    \n",
    "def get_authors_with_affiliation_name_initial(record, affiliations_to_check):\n",
    "    try:\n",
    "        authors = record['PubmedArticle'][0]['MedlineCitation']['Article']['AuthorList']\n",
    "        authors_with_affiliation_name_initial = []\n",
    "              \n",
    "        for author in authors:\n",
    "            author_affiliations = author.get('AffiliationInfo', [])\n",
    "\n",
    "            if not author_affiliations:\n",
    "                # If there's no specific affiliation information, check the overall affiliation field\n",
    "                affiliations = [author.get('Affiliation', '').lower()]\n",
    "            else:\n",
    "                # If there's specific affiliation information, extract and check each affiliation\n",
    "                affiliations = [affiliation.get('Affiliation', '').lower() for affiliation in author_affiliations]\n",
    "\n",
    "                \n",
    "\n",
    "            # Check if any phrase in affiliations_to_check is a substring of affiliation\n",
    "            if any(phrase.lower() in affiliation for phrase in affiliations_to_check for affiliation in affiliations):\n",
    "                initial_name = author.get('LastName', '') + ' ' + author.get('Initials', '')\n",
    "                authors_with_affiliation_name_initial.append(initial_name)\n",
    "                              \n",
    "        return authors_with_affiliation_name_initial if authors_with_affiliation_name_initial else None\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "                                                                             \n",
    "def get_authors_with_affiliation_affiliation(record, affiliations_to_check):\n",
    "    try:\n",
    "        authors = record['PubmedArticle'][0]['MedlineCitation']['Article']['AuthorList']\n",
    "        authors_with_affiliation_affiliation = []\n",
    "        for author in authors:\n",
    "            author_affiliations = author.get('AffiliationInfo', [])\n",
    "\n",
    "            if not author_affiliations:\n",
    "                affiliations = [author.get('Affiliation', '').lower()]\n",
    "            else:\n",
    "                affiliations = [affiliation.get('Affiliation', '').lower() for affiliation in author_affiliations]\n",
    "\n",
    "            if any(phrase.lower() in affiliation for phrase in affiliations_to_check for affiliation in affiliations):\n",
    "                authors_with_affiliation_affiliation.append(affiliations)\n",
    "\n",
    "        return authors_with_affiliation_affiliation if authors_with_affiliation_affiliation else None\n",
    "    except KeyError:\n",
    "        return None\n",
    "                                                                             \n",
    "def get_authors_with_affiliation_formatted(record, authors_with_affiliation):\n",
    "    #print(authors_with_affiliation)\n",
    "    if authors_with_affiliation:\n",
    "        return [f\"{author.split()[0]} {author.split()[1][0]}\" for author in authors_with_affiliation]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "        \n",
    "def get_all_affiliations(record):\n",
    "    try:\n",
    "        authors = record['PubmedArticle'][0]['MedlineCitation']['Article']['AuthorList']\n",
    "        all_affiliations = set()\n",
    "\n",
    "        for author in authors:\n",
    "            author_affiliations = author.get('AffiliationInfo', [])\n",
    "            \n",
    "            if not author_affiliations:\n",
    "                all_affiliations.add(author.get('Affiliation', '').lower())\n",
    "            else:\n",
    "                all_affiliations.update([affiliation.get('Affiliation', '').lower() for affiliation in author_affiliations])\n",
    "\n",
    "        return list(all_affiliations)\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def get_document_type(record):\n",
    "    try:\n",
    "        document_types = record['PubmedArticle'][0]['MedlineCitation']['Article']['PublicationTypeList']\n",
    "        return ', '.join(document_type for document_type in document_types)\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "   \n",
    "def process_pubmed_record(record, affiliations_to_check):\n",
    "    # Extract relevant information from the PubMed record\n",
    "    pubmed_data = {\n",
    "        \"Authors\": get_authors(record),\n",
    "        \"AuthorsWithAffiliationNameFull\": get_authors_with_affiliation_name_full(record, affiliations_to_check),\n",
    "        \"AuthorsWithAffiliationNameInitial\": get_authors_with_affiliation_name_initial(record, affiliations_to_check),\n",
    "        \"AuthorsWithAffiliationAffiliation\": get_authors_with_affiliation_affiliation(record, affiliations_to_check),\n",
    "        \"AllAffiliations\": get_all_affiliations(record),\n",
    "        \"Abstract\": get_abstract(record),\n",
    "        \"date_year\": get_publication_date_year(record),\n",
    "        \"date_monthY\": get_publication_date_month_year(record),\n",
    "        \"date_mdY\": get_publication_date_month_day_year(record),\n",
    "        \"PMID\": get_pmid_pmcid_doi(record)[0],\n",
    "        \"PMCID\": get_pmid_pmcid_doi(record)[1],\n",
    "        \"DOI\":get_pmid_pmcid_doi(record)[2],\n",
    "        \"JournalTitle\": get_journal_title(record),\n",
    "        \"ArticleTitle\": get_article_title_page(record)[0],\n",
    "        \"PageStart\": get_article_title_page(record)[1],\n",
    "        \"PageEnd\": get_article_title_page(record)[2],\n",
    "        \"Volume\": get_journal_volume_issue(record)[0],\n",
    "        \"Issue\": get_journal_volume_issue(record)[1],\n",
    "        \"DocumentType\": get_document_type(record)\n",
    "    }\n",
    "\n",
    "    return pubmed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84ea3e9c-33aa-4f2d-bc89-fe9664fd0ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\keg827\\AppData\\Local\\Temp\\ipykernel_31428\\1419410658.py:5: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  if os.path.exists('hiv not implementation\\pubmed_checkpoints'):\n",
      "C:\\Users\\keg827\\AppData\\Local\\Temp\\ipykernel_31428\\1419410658.py:6: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  shutil.rmtree('hiv not implementation\\pubmed_checkpoints')\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Clear out checkpoints if running notebook more than once\n",
    "# ========================================\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# CONFIGURATION - CHANGE THIS FOR EACH RUN\n",
    "# ========================================\n",
    "#INPUT_FOLDER = 'hiv imp'  \n",
    "INPUT_FOLDER = 'hiv not imp'  # Change to 'hiv imp' for the other dataset\n",
    "# ========================================\n",
    "\n",
    "# Delete checkpoint directory in the specified folder\n",
    "checkpoint_dir = os.path.join(INPUT_FOLDER, 'pubmed_checkpoints')\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    shutil.rmtree(checkpoint_dir)\n",
    "    print(f\"‚úì Checkpoint directory deleted: {checkpoint_dir}\")\n",
    "    print(\"Starting fresh - all checkpoints removed\")\n",
    "    \n",
    "    # Show what was in there before deletion\n",
    "    print(f\"\\nDeleted from: {INPUT_FOLDER}\")\n",
    "else:\n",
    "    print(f\"Directory not found: {checkpoint_dir}\")\n",
    "    print(f\"\\nAvailable directories in {INPUT_FOLDER}:\")\n",
    "    if os.path.exists(INPUT_FOLDER):\n",
    "        for item in os.listdir(INPUT_FOLDER):\n",
    "            if os.path.isdir(os.path.join(INPUT_FOLDER, item)):\n",
    "                print(f\"  üìÅ {item}\")\n",
    "    else:\n",
    "        print(f\"  ERROR: Folder '{INPUT_FOLDER}' doesn't exist!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a782f55-311a-431e-a9ea-ec0a0ae14c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CHUNK 1/51: 2000/01/01 to 2000/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 22\n",
      "WebEnv: MCID_6930ecc6a245bf9...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 22 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c4ca14022a4b7ead44f8fec42c2f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 1 complete: 22 records saved to hiv_imp_chunk_01_2000-01-01_2000-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 2/51: 2000/07/01 to 2000/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 12\n",
      "WebEnv: MCID_6930ecca0da1ec2...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 12 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec60a17127840aab2e5b2f56e303794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 2 complete: 12 records saved to hiv_imp_chunk_02_2000-07-01_2000-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 3/51: 2001/01/01 to 2001/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 11\n",
      "WebEnv: MCID_6930eccdbc26cbd...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 11 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f790404527a549e1945f81e993272955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 3 complete: 11 records saved to hiv_imp_chunk_03_2001-01-01_2001-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 4/51: 2001/07/01 to 2001/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 7\n",
      "WebEnv: MCID_6930ecd08ff117b...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 7 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3518e65fd74222bb6f8cb854ac8677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 4 complete: 7 records saved to hiv_imp_chunk_04_2001-07-01_2001-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 5/51: 2002/01/01 to 2002/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 12\n",
      "WebEnv: MCID_6930ecd466b8716...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 12 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8403ce7fdb1842b2b5093ad71d32f891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 5 complete: 12 records saved to hiv_imp_chunk_05_2002-01-01_2002-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 6/51: 2002/07/01 to 2002/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 17\n",
      "WebEnv: MCID_6930ecd7754c56d...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 17 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e09904571449a7936d52892b10bdca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 6 complete: 17 records saved to hiv_imp_chunk_06_2002-07-01_2002-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 7/51: 2003/01/01 to 2003/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 18\n",
      "WebEnv: MCID_6930ecda98217a2...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 18 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d5b2e57d1247cd815e68f2164db49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 7 complete: 18 records saved to hiv_imp_chunk_07_2003-01-01_2003-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 8/51: 2003/07/01 to 2003/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 9\n",
      "WebEnv: MCID_6930ecdeb347068...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 9 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de2d550036d499885b873cfaa4a36bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 8 complete: 9 records saved to hiv_imp_chunk_08_2003-07-01_2003-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 9/51: 2004/01/01 to 2004/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 23\n",
      "WebEnv: MCID_6930ece14985343...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 23 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd904536ca2e41fdb127298fd10b14af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 9 complete: 23 records saved to hiv_imp_chunk_09_2004-01-01_2004-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 10/51: 2004/07/01 to 2004/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 13\n",
      "WebEnv: MCID_6930ece54c1c56c...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 13 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04046cedb9bb4426ad4581bd40a79754",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 10 complete: 13 records saved to hiv_imp_chunk_10_2004-07-01_2004-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 11/51: 2005/01/01 to 2005/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 18\n",
      "WebEnv: MCID_6930ece80c15cc2...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 18 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5170811eacc0402ba3cb6618aa64034a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 11 complete: 18 records saved to hiv_imp_chunk_11_2005-01-01_2005-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 12/51: 2005/07/01 to 2005/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 21\n",
      "WebEnv: MCID_6930ecebdca5a81...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 21 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cd130d58a6441ca5c6399122409cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 12 complete: 21 records saved to hiv_imp_chunk_12_2005-07-01_2005-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 13/51: 2006/01/01 to 2006/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 16\n",
      "WebEnv: MCID_6930ecefa1a0092...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 16 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efc743108b849cfa307e5e5b65a02d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 13 complete: 16 records saved to hiv_imp_chunk_13_2006-01-01_2006-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 14/51: 2006/07/01 to 2006/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 41\n",
      "WebEnv: MCID_6930ecf27bb1972...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 41 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083a84ff16594d16a584939c8a7f2a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 14 complete: 41 records saved to hiv_imp_chunk_14_2006-07-01_2006-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 15/51: 2007/01/01 to 2007/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 28\n",
      "WebEnv: MCID_6930ecf526883e9...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 28 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d29db97394499892c176a0ecb6d870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 15 complete: 28 records saved to hiv_imp_chunk_15_2007-01-01_2007-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 16/51: 2007/07/01 to 2007/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 28\n",
      "WebEnv: MCID_6930ecf9863d050...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 28 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ad7c25d3c94578b5aa898ed119f256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 16 complete: 28 records saved to hiv_imp_chunk_16_2007-07-01_2007-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 17/51: 2008/01/01 to 2008/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 23\n",
      "WebEnv: MCID_6930ecfc85e1e33...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 23 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ab8c80985541dd81fa7905d259032f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 17 complete: 23 records saved to hiv_imp_chunk_17_2008-01-01_2008-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 18/51: 2008/07/01 to 2008/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 26\n",
      "WebEnv: MCID_6930ed005538445...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 26 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513cafa3c33446c8916ffc6e311ac4ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 18 complete: 26 records saved to hiv_imp_chunk_18_2008-07-01_2008-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 19/51: 2009/01/01 to 2009/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 31\n",
      "WebEnv: MCID_6930ed03870cf0a...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 31 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2723241224884393b45378e1033f5c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 19 complete: 30 records saved to hiv_imp_chunk_19_2009-01-01_2009-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 20/51: 2009/07/01 to 2009/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 37\n",
      "WebEnv: MCID_6930ed0801f4c41...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 37 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eacf07492b43491696564db96c705549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 20 complete: 37 records saved to hiv_imp_chunk_20_2009-07-01_2009-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 21/51: 2010/01/01 to 2010/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 46\n",
      "WebEnv: MCID_6930ed0be666928...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 46 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d30078125242069f1c1408625e2942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 21 complete: 45 records saved to hiv_imp_chunk_21_2010-01-01_2010-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 22/51: 2010/07/01 to 2010/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 43\n",
      "WebEnv: MCID_6930ed0fe3c64ba...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 43 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4bfc66502e84bff80a2d1b4d12bfad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 22 complete: 43 records saved to hiv_imp_chunk_22_2010-07-01_2010-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 23/51: 2011/01/01 to 2011/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 50\n",
      "WebEnv: MCID_6930ed139cce85a...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 50 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb28b4624ed0431fa2ddf8b6a3f98545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 23 complete: 50 records saved to hiv_imp_chunk_23_2011-01-01_2011-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 24/51: 2011/07/01 to 2011/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 45\n",
      "WebEnv: MCID_6930ed1601e0d3b...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 45 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90eec3dac698467ba5e28a9b1b67e06f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 24 complete: 45 records saved to hiv_imp_chunk_24_2011-07-01_2011-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 25/51: 2012/01/01 to 2012/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 53\n",
      "WebEnv: MCID_6930ed1a80b828e...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 53 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbf1c0ce0f744d6ba442fd59f93a77f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 25 complete: 53 records saved to hiv_imp_chunk_25_2012-01-01_2012-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 26/51: 2012/07/01 to 2012/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 41\n",
      "WebEnv: MCID_6930ed1da2b481f...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 41 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1aeecee2e648899ef0cf9a993bef62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 26 complete: 41 records saved to hiv_imp_chunk_26_2012-07-01_2012-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 27/51: 2013/01/01 to 2013/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 62\n",
      "WebEnv: MCID_6930ed21a41f3ba...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 62 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fec68cfe7c4da29d737939e3d23108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 27 complete: 62 records saved to hiv_imp_chunk_27_2013-01-01_2013-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 28/51: 2013/07/01 to 2013/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 50\n",
      "WebEnv: MCID_6930ed26a45350e...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 50 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86006de76c240b8848c898576447a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 28 complete: 50 records saved to hiv_imp_chunk_28_2013-07-01_2013-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 29/51: 2014/01/01 to 2014/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 80\n",
      "WebEnv: MCID_6930ed2b6176e1a...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 80 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c968623e02ad4b29bc9a39846df4b5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 29 complete: 80 records saved to hiv_imp_chunk_29_2014-01-01_2014-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 30/51: 2014/07/01 to 2014/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 66\n",
      "WebEnv: MCID_6930ed30edac90a...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 66 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c875fa8de1ef4f5abcca4f29c0dbe0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 30 complete: 66 records saved to hiv_imp_chunk_30_2014-07-01_2014-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 31/51: 2015/01/01 to 2015/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 70\n",
      "WebEnv: MCID_6930ed34c9631e1...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 70 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "944a2f60e10e43d191b59363a6302730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 31 complete: 70 records saved to hiv_imp_chunk_31_2015-01-01_2015-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 32/51: 2015/07/01 to 2015/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 69\n",
      "WebEnv: MCID_6930ed3949af0e4...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 69 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c82f1bc6f7145168a0ab649ec63c545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 32 complete: 69 records saved to hiv_imp_chunk_32_2015-07-01_2015-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 33/51: 2016/01/01 to 2016/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 111\n",
      "WebEnv: MCID_6930ed3f803867d...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 111 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7fba6bd33fd458b9f6e95faa10d503a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 33 complete: 111 records saved to hiv_imp_chunk_33_2016-01-01_2016-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 34/51: 2016/07/01 to 2016/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 72\n",
      "WebEnv: MCID_6930ed44778dbdd...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 72 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6d5b39a1054baa8b336c063d954c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 34 complete: 72 records saved to hiv_imp_chunk_34_2016-07-01_2016-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 35/51: 2017/01/01 to 2017/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 88\n",
      "WebEnv: MCID_6930ed49b38f600...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 88 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844869d12c204bd6aa632ba463c5b80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 35 complete: 88 records saved to hiv_imp_chunk_35_2017-01-01_2017-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 36/51: 2017/07/01 to 2017/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 71\n",
      "WebEnv: MCID_6930ed4ed632a91...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 71 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbc73497cb44a34ad658c398be6d6af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 36 complete: 70 records saved to hiv_imp_chunk_36_2017-07-01_2017-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 37/51: 2018/01/01 to 2018/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 90\n",
      "WebEnv: MCID_6930ed537c7ed36...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 90 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0ecb66522c49ee9621b2954bcfeb56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 37 complete: 89 records saved to hiv_imp_chunk_37_2018-01-01_2018-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 38/51: 2018/07/01 to 2018/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 88\n",
      "WebEnv: MCID_6930ed5920efb7c...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 88 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e35683ebac94ba58ceb037d59fb613e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 38 complete: 88 records saved to hiv_imp_chunk_38_2018-07-01_2018-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 39/51: 2019/01/01 to 2019/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 96\n",
      "WebEnv: MCID_6930ed5d0a52b6d...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 96 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0860b44d2346639e54e129f01e6a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 39 complete: 96 records saved to hiv_imp_chunk_39_2019-01-01_2019-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 40/51: 2019/07/01 to 2019/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 113\n",
      "WebEnv: MCID_6930ed62e1c5005...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 113 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dccc04d00a64702875797c33f09808c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 40 complete: 113 records saved to hiv_imp_chunk_40_2019-07-01_2019-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 41/51: 2020/01/01 to 2020/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 123\n",
      "WebEnv: MCID_6930ed675538445...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 123 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5309d53b6624093a6e357d5702787c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 41 complete: 123 records saved to hiv_imp_chunk_41_2020-01-01_2020-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 42/51: 2020/07/01 to 2020/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 103\n",
      "WebEnv: MCID_6930ed6c07fbe43...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 103 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27cbbb42f9f427caaff70694b2fd7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 42 complete: 103 records saved to hiv_imp_chunk_42_2020-07-01_2020-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 43/51: 2021/01/01 to 2021/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 143\n",
      "WebEnv: MCID_6930ed715376b67...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 143 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db97ba9554b4dc19343bd7969362cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 43 complete: 143 records saved to hiv_imp_chunk_43_2021-01-01_2021-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 44/51: 2021/07/01 to 2021/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 106\n",
      "WebEnv: MCID_6930ed750890e40...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 106 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a772cb708e84483b92b60b0d7c69ddef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 44 complete: 106 records saved to hiv_imp_chunk_44_2021-07-01_2021-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 45/51: 2022/01/01 to 2022/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 130\n",
      "WebEnv: MCID_6930ed7ab64a346...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 130 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70640cc8dc34493cb2e353a1af3235c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 45 complete: 130 records saved to hiv_imp_chunk_45_2022-01-01_2022-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 46/51: 2022/07/01 to 2022/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 174\n",
      "WebEnv: MCID_6930ed808c03c9e...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 174 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a82275dbcd264f73aac7b4585f3e00a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 46 complete: 174 records saved to hiv_imp_chunk_46_2022-07-01_2022-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 47/51: 2023/01/01 to 2023/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 159\n",
      "WebEnv: MCID_6930ed8566b8716...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 159 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea0c15935a014512aecc96f6cd6a9164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 47 complete: 159 records saved to hiv_imp_chunk_47_2023-01-01_2023-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 48/51: 2023/07/01 to 2023/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 157\n",
      "WebEnv: MCID_6930ed8bc248ca2...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 157 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96c77a0d32d4c6d936b25e82800ab69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 48 complete: 157 records saved to hiv_imp_chunk_48_2023-07-01_2023-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 49/51: 2024/01/01 to 2024/06/30\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 124\n",
      "WebEnv: MCID_6930ed90bd5b3bd...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 124 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be5580a2e2d49e688aa0be94739841c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 49 complete: 124 records saved to hiv_imp_chunk_49_2024-01-01_2024-06-30.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 50/51: 2024/07/01 to 2024/12/31\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 127\n",
      "WebEnv: MCID_6930ed96a56d289...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 127 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f4db05548d41a390848a2e791e767e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 50 complete: 127 records saved to hiv_imp_chunk_50_2024-07-01_2024-12-31.csv\n",
      "\n",
      "======================================================================\n",
      "CHUNK 51/51: 2025/01/01 to 2025/12/03\n",
      "======================================================================\n",
      "\n",
      "Posting search to NCBI history server...\n",
      "Total results: 305\n",
      "WebEnv: MCID_6930ed9b225bb2f...\n",
      "QueryKey: 1\n",
      "\n",
      "Processing 305 records in batches of 200...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c7078f35c0453ebc900f77b18f1e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing records:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Chunk 51 complete: 304 records saved to hiv_imp_chunk_51_2025-01-01_2025-12-03.csv\n",
      "\n",
      "======================================================================\n",
      "COMBINING ALL CHUNKS\n",
      "======================================================================\n",
      "\n",
      "Loaded hiv_imp_chunk_01_2000-01-01_2000-06-30.csv: 22 records\n",
      "Loaded hiv_imp_chunk_02_2000-07-01_2000-12-31.csv: 12 records\n",
      "Loaded hiv_imp_chunk_03_2001-01-01_2001-06-30.csv: 11 records\n",
      "Loaded hiv_imp_chunk_04_2001-07-01_2001-12-31.csv: 7 records\n",
      "Loaded hiv_imp_chunk_05_2002-01-01_2002-06-30.csv: 12 records\n",
      "Loaded hiv_imp_chunk_06_2002-07-01_2002-12-31.csv: 17 records\n",
      "Loaded hiv_imp_chunk_07_2003-01-01_2003-06-30.csv: 18 records\n",
      "Loaded hiv_imp_chunk_08_2003-07-01_2003-12-31.csv: 9 records\n",
      "Loaded hiv_imp_chunk_09_2004-01-01_2004-06-30.csv: 23 records\n",
      "Loaded hiv_imp_chunk_10_2004-07-01_2004-12-31.csv: 13 records\n",
      "Loaded hiv_imp_chunk_11_2005-01-01_2005-06-30.csv: 18 records\n",
      "Loaded hiv_imp_chunk_12_2005-07-01_2005-12-31.csv: 21 records\n",
      "Loaded hiv_imp_chunk_13_2006-01-01_2006-06-30.csv: 16 records\n",
      "Loaded hiv_imp_chunk_14_2006-07-01_2006-12-31.csv: 41 records\n",
      "Loaded hiv_imp_chunk_15_2007-01-01_2007-06-30.csv: 28 records\n",
      "Loaded hiv_imp_chunk_16_2007-07-01_2007-12-31.csv: 28 records\n",
      "Loaded hiv_imp_chunk_17_2008-01-01_2008-06-30.csv: 23 records\n",
      "Loaded hiv_imp_chunk_18_2008-07-01_2008-12-31.csv: 26 records\n",
      "Loaded hiv_imp_chunk_19_2009-01-01_2009-06-30.csv: 30 records\n",
      "Loaded hiv_imp_chunk_20_2009-07-01_2009-12-31.csv: 37 records\n",
      "Loaded hiv_imp_chunk_21_2010-01-01_2010-06-30.csv: 45 records\n",
      "Loaded hiv_imp_chunk_22_2010-07-01_2010-12-31.csv: 43 records\n",
      "Loaded hiv_imp_chunk_23_2011-01-01_2011-06-30.csv: 50 records\n",
      "Loaded hiv_imp_chunk_24_2011-07-01_2011-12-31.csv: 45 records\n",
      "Loaded hiv_imp_chunk_25_2012-01-01_2012-06-30.csv: 53 records\n",
      "Loaded hiv_imp_chunk_26_2012-07-01_2012-12-31.csv: 41 records\n",
      "Loaded hiv_imp_chunk_27_2013-01-01_2013-06-30.csv: 62 records\n",
      "Loaded hiv_imp_chunk_28_2013-07-01_2013-12-31.csv: 50 records\n",
      "Loaded hiv_imp_chunk_29_2014-01-01_2014-06-30.csv: 80 records\n",
      "Loaded hiv_imp_chunk_30_2014-07-01_2014-12-31.csv: 66 records\n",
      "Loaded hiv_imp_chunk_31_2015-01-01_2015-06-30.csv: 70 records\n",
      "Loaded hiv_imp_chunk_32_2015-07-01_2015-12-31.csv: 69 records\n",
      "Loaded hiv_imp_chunk_33_2016-01-01_2016-06-30.csv: 111 records\n",
      "Loaded hiv_imp_chunk_34_2016-07-01_2016-12-31.csv: 72 records\n",
      "Loaded hiv_imp_chunk_35_2017-01-01_2017-06-30.csv: 88 records\n",
      "Loaded hiv_imp_chunk_36_2017-07-01_2017-12-31.csv: 70 records\n",
      "Loaded hiv_imp_chunk_37_2018-01-01_2018-06-30.csv: 89 records\n",
      "Loaded hiv_imp_chunk_38_2018-07-01_2018-12-31.csv: 88 records\n",
      "Loaded hiv_imp_chunk_39_2019-01-01_2019-06-30.csv: 96 records\n",
      "Loaded hiv_imp_chunk_40_2019-07-01_2019-12-31.csv: 113 records\n",
      "Loaded hiv_imp_chunk_41_2020-01-01_2020-06-30.csv: 123 records\n",
      "Loaded hiv_imp_chunk_42_2020-07-01_2020-12-31.csv: 103 records\n",
      "Loaded hiv_imp_chunk_43_2021-01-01_2021-06-30.csv: 143 records\n",
      "Loaded hiv_imp_chunk_44_2021-07-01_2021-12-31.csv: 106 records\n",
      "Loaded hiv_imp_chunk_45_2022-01-01_2022-06-30.csv: 130 records\n",
      "Loaded hiv_imp_chunk_46_2022-07-01_2022-12-31.csv: 174 records\n",
      "Loaded hiv_imp_chunk_47_2023-01-01_2023-06-30.csv: 159 records\n",
      "Loaded hiv_imp_chunk_48_2023-07-01_2023-12-31.csv: 157 records\n",
      "Loaded hiv_imp_chunk_49_2024-01-01_2024-06-30.csv: 124 records\n",
      "Loaded hiv_imp_chunk_50_2024-07-01_2024-12-31.csv: 127 records\n",
      "Loaded hiv_imp_chunk_51_2025-01-01_2025-12-03.csv: 304 records\n",
      "\n",
      "======================================================================\n",
      "‚úì PROCESSING COMPLETE!\n",
      "======================================================================\n",
      "Total chunks processed: 51\n",
      "Total unique records: 2,887\n",
      "Final output: hiv_imp_us_2000_2025_FINAL.csv\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Query PubMed via Entrez API\n",
    "# ========================================\n",
    "\n",
    "# ========================================\n",
    "# CONFIGURATION - CHANGE THIS FOR EACH RUN\n",
    "# ========================================\n",
    "#OUTPUT_FOLDER = 'hiv imp'  # Change to 'hiv not imp' for the other dataset\n",
    "OUTPUT_FOLDER = 'hiv not imp'  # Change to 'hiv not imp' for the other dataset\n",
    "# ========================================\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Set up logging (save to folder)\n",
    "log_file = os.path.join(OUTPUT_FOLDER, 'pubmed_errors.log')\n",
    "logging.basicConfig(\n",
    "    filename=log_file, \n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "Entrez.email = ENTREZ_EMAIL\n",
    "Entrez.api_key = ENTREZ_API_KEY\n",
    "\n",
    "# Checkpoint directory (inside output folder)\n",
    "CHECKPOINT_DIR = os.path.join(OUTPUT_FOLDER, \"pubmed_checkpoints\")\n",
    "CHECKPOINT_INTERVAL = 50\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "def save_checkpoint(batch_index, pubmed_data, failed_batches, total_count):\n",
    "    checkpoint = {\n",
    "        'batch_index': batch_index,\n",
    "        'pubmed_data': pubmed_data,\n",
    "        'failed_batches': failed_batches,\n",
    "        'total_count': total_count,\n",
    "        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    checkpoint_file = os.path.join(CHECKPOINT_DIR, f'hiv_imp_checkpoint_{batch_index}.pkl')\n",
    "    with open(checkpoint_file, 'wb') as f:\n",
    "        pickle.dump(checkpoint, f)\n",
    "    \n",
    "    if pubmed_data:\n",
    "        temp_df = pd.DataFrame(pubmed_data)\n",
    "        csv_file = os.path.join(CHECKPOINT_DIR, f'data_hiv_imp_{batch_index}.csv')\n",
    "        temp_df.to_csv(csv_file, index=False)\n",
    "    \n",
    "    logging.info(f\"Checkpoint saved at record {batch_index}: {len(pubmed_data)} records\")\n",
    "\n",
    "def load_latest_checkpoint():\n",
    "    if not os.path.exists(CHECKPOINT_DIR):\n",
    "        return None\n",
    "    \n",
    "    checkpoint_files = [f for f in os.listdir(CHECKPOINT_DIR) if f.startswith('checkpoint_') and f.endswith('.pkl')]\n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    \n",
    "    latest_file = max(checkpoint_files, key=lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "    checkpoint_path = os.path.join(CHECKPOINT_DIR, latest_file)\n",
    "    \n",
    "    with open(checkpoint_path, 'rb') as f:\n",
    "        checkpoint = pickle.load(f)\n",
    "    \n",
    "    logging.info(f\"Loaded checkpoint from record {checkpoint['batch_index']}: {len(checkpoint['pubmed_data'])} records\")\n",
    "    return checkpoint\n",
    "\n",
    "def fetch_records_from_history(webenv, query_key, retstart, retmax, max_retries=3):\n",
    "    \"\"\"Fetch records using the history server\"\"\"\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            handle = Entrez.efetch(\n",
    "                db=\"pubmed\",\n",
    "                rettype=\"xml\",\n",
    "                retmode=\"xml\",\n",
    "                retstart=retstart,\n",
    "                retmax=retmax,\n",
    "                webenv=webenv,\n",
    "                query_key=query_key\n",
    "            )\n",
    "            records = Entrez.read(handle)\n",
    "            handle.close()\n",
    "            return records\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            wait_time = 2 ** retries\n",
    "            logging.error(f\"Error fetching records at position {retstart} (attempt {retries}/{max_retries}): {e}\")\n",
    "            time.sleep(wait_time)\n",
    "            if retries == max_retries:\n",
    "                logging.error(f\"Failed after {max_retries} retries\")\n",
    "                return None\n",
    "\n",
    "# ========================================\n",
    "# BASE QUERY with {DATE_FILTER} placeholder\n",
    "# ========================================\n",
    "# HIV NOT implementation\n",
    "#base_query = \"\"\"(HIV[ti] OR HIV1[ti] OR HIVI[ti] OR HIVII[ti] OR HIV2[ti] OR human-immuno-deficiency-virus*[ti] OR human-immunodeficiency-virus*[ti] OR acquired-immune-deficiency-syndrome*[ti] OR acquired-immunodeficiency-syndrome*[ti] OR acquired-immuno-deficiency-syndrome*[ti] OR acute-retroviral-syndrome[ti] OR acute-seroconversion-syndrome[ti] OR Highly-Active-Antiretroviral-Therapy[ti] OR HAART[ti] OR PLWH[ti] OR WLWH[ti] OR MLWH[ti] OR ALWH[ti] OR PLHIV[ti] OR PLWHIV[ti] OR aids-related[ti] OR aids-infected[ti] OR aids-affected[ti] OR living-with-aids[ti] OR patients-with-aids[ti] OR people-with-aids[ti] OR women-with-aids[ti] OR men-with-aids[ti] OR aids-patients[ti] OR \"AIDS\"[Journal] OR \"AIDS Behav\"[Journal] OR \"AIDS Care\"[Journal] OR \"AIDS Educ Prev\"[Journal] OR \"AIDS Patient Care STDS\"[Journal] OR \"AIDS Res Hum Retroviruses\"[Journal] OR \"AIDS Res Ther\"[Journal] OR \"AIDS Rev\"[Journal] OR \"Curr HIV/AIDS Rep\"[Journal] OR \"Curr HIV Res\"[Journal] OR \"Curr Opin HIV AIDS\"[Journal] OR \"HIV Med\"[Journal] OR \"HIV Res Clin Pract\"[Journal] OR \"Int J STD AIDS\"[Journal] OR \"J Acquir Immune Defic Syndr\"[Journal] OR \"J Assoc Nurses AIDS Care\"[Journal] OR \"J Int AIDS Soc\"[Journal] OR \"Lancet HIV\"[Journal] OR \"J Int Assoc Provid AIDS Care\"[Journal] OR \"SAHARA J\"[Journal] OR (Aids[ti] NOT (decision-aid*[ti] OR decision-support-aid*[ti] OR decisionmaking-aid*[ti] OR decision-making-aid*[ti] OR hearing-aid*[ti] OR auditory-aid*[ti] OR audiovisual-aid*[ti] OR visual-aid*[ti] OR sensory-aid*[ti] OR communication-aid*[ti] OR cognitive-aid*[ti] OR mobility-aid*[ti] OR walking-aid*[ti] OR sleep-aid*[ti] OR vision-aid*[ti] OR cessation-aid*[ti] OR conversation-aid*[ti] OR memory-aid*[ti] OR diagnostic-aid*[ti] OR pharmacologic-aid*[ti] OR pharmaceutical-aid*[ti] OR medication-aid*[ti] OR job-aid*[ti] OR visualization-aid*[ti] OR pictorial-aid*[ti] OR training-aid*[ti] OR rehabilitation-aid*[ti]))) AND (english[lang] AND {DATE_FILTER} NOT ((africa[Mesh] OR \"caribbean region\"[Mesh] OR \"central america\"[Mesh] OR \"latin america\"[Mesh:NoExp] OR canada[Mesh] OR greenland[Mesh:NoExp] OR mexico[Mesh:NoExp] OR \"south america\"[Mesh] OR \"antarctic regions\"[Mesh:NoExp] OR \"arctic regions\"[Mesh:NoExp] OR asia[Mesh] OR europe[Mesh] OR oceania[Mesh] OR \"afghanistan\"[ti] OR \"africa\"[ti] OR \"albania\"[ti] OR \"algeria\"[ti] OR \"american samoa\"[ti] OR \"andorra\"[ti] OR \"angola\"[ti] OR \"anguilla\"[ti] OR \"antarctica\"[ti] OR \"antigua and barbuda\"[ti] OR \"argentina\"[ti] OR \"armenia\"[ti] OR \"aruba\"[ti] OR \"asia\"[ti] OR \"australia\"[ti] OR \"austria\"[ti] OR \"azerbaijan\"[ti] OR \"bahamas\"[ti] OR \"bahrain\"[ti] OR \"balkan states\"[ti] OR \"baltic states\"[ti] OR \"bangladesh\"[ti] OR \"barbados\"[ti] OR \"belarus\"[ti] OR \"belgium\"[ti] OR \"belize\"[ti] OR \"benin\"[ti] OR \"bermuda\"[ti] OR \"bhutan\"[ti] OR \"bolivia\"[ti] OR \"bosnia-herzegovina\"[ti] OR \"botswana\"[ti] OR \"bouvet\"[ti] OR \"brazil\"[ti] OR \"brunei\"[ti] OR \"bulgaria\"[ti] OR \"burkina faso\"[ti] OR \"burundi\"[ti] OR \"cambodia\"[ti] OR \"cameroon\"[ti] OR \"canada\"[ti] OR \"cape verde\"[ti] OR \"central african republic\"[ti] OR \"central america\"[ti] OR \"chad\"[ti] OR \"chile\"[ti] OR \"china\"[ti] OR \"colombia\"[ti] OR \"commonwealth of independent states\"[ti] OR \"comoros\"[ti] OR \"cook\"[ti] OR \"coral sea\"[ti] OR \"costa rica\"[ti] OR \"croatia\"[ti] OR \"cuba\"[ti] OR \"curacao\"[ti] OR \"cyprus\"[ti] OR \"czech republic\"[ti] OR \"czechoslovakia\"[ti] OR \"denmark\"[ti] OR \"djibouti\"[ti] OR \"dominica\"[ti] OR \"dominican republic\"[ti] OR \"east timor\"[ti] OR \"eastern europe\"[ti] OR \"ecuador\"[ti] OR \"egypt\"[ti] OR \"el salvador\"[ti] OR \"england\"[ti] OR \"equatorial guinea\"[ti] OR \"eritrea\"[ti] OR \"estonia\"[ti] OR \"ethiopia\"[ti] OR \"europa\"[ti] OR \"europe\"[ti] OR \"falkland\"[ti] OR \"faroe\"[ti] OR \"fiji\"[ti] OR \"finland\"[ti] OR \"france\"[ti] OR \"french guiana\"[ti] OR \"french polynesia\"[ti] OR \"gabon\"[ti] OR \"gambia\"[ti] OR \"gaza strip\"[ti] OR \"georgia\"[ti] OR \"germany\"[ti] OR \"ghana\"[ti] OR \"gibraltar\"[ti] OR \"great britain\"[ti] OR \"greece\"[ti] OR \"greenland\"[ti] OR \"grenada\"[ti] OR \"guadeloupe\"[ti] OR \"guam\"[ti] OR \"guatemala\"[ti] OR \"guernsey\"[ti] OR \"guinea\"[ti] OR \"guyana\"[ti] OR \"haiti\"[ti] OR \"honduras\"[ti] OR \"hong kong\"[ti] OR \"hungary\"[ti] OR \"iceland\"[ti] OR \"india\"[ti] OR \"indonesia\"[ti] OR \"iran\"[ti] OR \"iraq\"[ti] OR \"ireland\"[ti] OR \"israel\"[ti] OR \"italy\"[ti] OR \"ivory coast\"[ti] OR \"jamaica\"[ti] OR \"jan mayen\"[ti] OR \"japan\"[ti] OR \"jersey\"[ti] OR \"johnston atoll\"[ti] OR \"jordan\"[ti] OR \"juan de nova\"[ti] OR \"kazakhstan\"[ti] OR \"kenya\"[ti] OR \"kiribati\"[ti] OR \"korea\"[ti] OR \"kosovo\"[ti] OR \"kuwait\"[ti] OR \"kyrgyzstan\"[ti] OR \"laos\"[ti] OR \"latin america\"[ti] OR \"latvia\"[ti] OR \"lebanon\"[ti] OR \"lesotho\"[ti] OR \"liberia\"[ti] OR \"libya\"[ti] OR \"liechtenstein\"[ti] OR \"lithuania\"[ti] OR \"luxembourg\"[ti] OR \"macao\"[ti] OR \"macedonia\"[ti] OR \"madagascar\"[ti] OR \"malawi\"[ti] OR \"malaysia\"[ti] OR \"maldives\"[ti] OR \"mali\"[ti] OR \"malta\"[ti] OR \"marshall\"[ti] OR \"martinique\"[ti] OR \"mauritania\"[ti] OR \"mauritius\"[ti] OR \"mexico\"[ti] OR \"micronesia\"[ti] OR \"middle east\"[ti] OR \"midway\"[ti] OR \"moldova\"[ti] OR \"monaco\"[ti] OR \"mongolia\"[ti] OR \"montserrat\"[ti] OR \"morocco\"[ti] OR \"mozambique\"[ti] OR \"myanmar\"[ti] OR \"namibia\"[ti] OR \"nauru\"[ti] OR \"navassa\"[ti] OR \"nepal\"[ti] OR \"netherlands\"[ti] OR \"netherlands antilles\"[ti] OR \"new caledonia\"[ti] OR \"new zealand\"[ti] OR \"nicaragua\"[ti] OR \"niger\"[ti] OR \"nigeria\"[ti] OR \"niue\"[ti] OR \"norfolk\"[ti] OR \"north macedonia\"[ti] OR \"northern ireland\"[ti] OR \"northern mariana\"[ti] OR \"norway\"[ti] OR \"oman\"[ti] OR \"pakistan\"[ti] OR \"palau\"[ti] OR \"palestine\"[ti] OR \"panama\"[ti] OR \"papua new guinea\"[ti] OR \"paraguay\"[ti] OR \"peru\"[ti] OR \"philippines\"[ti] OR \"pitcairn\"[ti] OR \"poland\"[ti] OR \"portugal\"[ti] OR \"puerto rico\"[ti] OR \"qatar\"[ti] OR \"reunion\"[ti] OR \"romania\"[ti] OR \"russia\"[ti] OR \"rwanda\"[ti] OR \"saint helena\"[ti] OR \"saint kitts and nevis\"[ti] OR \"saint lucia\"[ti] OR \"saint pierre and miquelon\"[ti] OR \"saint vincent and the grenadines\"[ti] OR \"samoa\"[ti] OR \"san marino\"[ti] OR \"sao tome and principe\"[ti] OR \"saudi arabia\"[ti] OR \"senegal\"[ti] OR \"serbia\"[ti] OR \"seychelles\"[ti] OR \"sierra leone\"[ti] OR \"singapore\"[ti] OR \"slovakia\"[ti] OR \"slovenia\"[ti] OR \"solomon\"[ti] OR \"somalia\"[ti] OR \"south africa\"[ti] OR \"south america\"[ti] OR \"south korea\"[ti] OR \"spain\"[ti] OR \"sri lanka\"[ti] OR \"sudan\"[ti] OR \"suriname\"[ti] OR \"svalbard\"[ti] OR \"swaziland\"[ti] OR \"sweden\"[ti] OR \"switzerland\"[ti] OR \"syria\"[ti] OR \"taiwan\"[ti] OR \"tajikistan\"[ti] OR \"tanzania\"[ti] OR \"thailand\"[ti] OR \"togo\"[ti] OR \"tokelau\"[ti] OR \"tonga\"[ti] OR \"Trinidad\"[ti] OR \"tobago\"[ti] OR \"tunisia\"[ti] OR \"turkey\"[ti] OR \"turkmenistan\"[ti] OR \"tuvalu\"[ti] OR \"uganda\"[ti] OR \"ukraine\"[ti] OR \"united arab emirates\"[ti] OR \"united kingdom\"[ti] OR \"united states\"[ti] OR \"uruguay\"[ti] OR \"uzbekistan\"[ti] OR \"vanuatu\"[ti] OR \"vatican city\"[ti] OR \"venezuela\"[ti] OR \"vietnam\"[ti] OR \"virgin\"[ti] OR \"wallis and futuna\"[ti] OR \"west bank\"[ti] OR \"western sahara\"[ti] OR \"yemen\"[ti] OR \"yugoslavia\"[ti] OR \"zaire\"[ti] OR \"zambia\"[ti] OR \"zimbabwe\"[ti]) NOT (\"north america\"[Mesh:NoExp] OR \"united states\"[Mesh] OR \"north American people\"[Mesh:NoExp] OR \"American indian or alaska native\"[Mesh] OR \"population groups, us\"[Mesh:NoExp] OR north-america*[tiab] OR united-states[tiab] OR usa[tiab] OR us[tiab] OR \"u s a\"[tiab] OR \"u s\"[tiab] OR alabama[tiab] OR alaska[tiab] OR arizona[tiab] OR arkansas[tiab] OR california[tiab] OR colorado[tiab] OR connecticut[tiab] OR delaware[tiab] OR district-of-columbia[tiab] OR florida[tiab] OR georgia[tiab] OR hawaii[tiab] OR idaho[tiab] OR illinois[tiab] OR indiana[tiab] OR iowa[tiab] OR kansas[tiab] OR kentucky[tiab] OR louisiana[tiab] OR maine[tiab] OR maryland[tiab] OR massachusetts[tiab] OR michigan[tiab] OR minnesota[tiab] OR mississippi[tiab] OR missouri[tiab] OR montana[tiab] OR nebraska[tiab] OR nevada[tiab] OR new-hampshire[tiab] OR new-jersey[tiab] OR new-mexico[tiab] OR new-york[tiab] OR north-carolina[tiab] OR north-dakota[tiab] OR ohio[tiab] OR oklahoma[tiab] OR oregon[tiab] OR pennsylvania[tiab] OR rhode-island[tiab] OR south-carolina[tiab] OR south-dakota[tiab] OR tennessee[tiab] OR texas[tiab] OR utah[tiab] OR vermont[tiab] OR virginia[tiab] OR washington[tiab] OR west-virginia[tiab] OR wisconsin[tiab] OR wyoming[tiab]))) NOT (\"Implementation Science\"[majr] OR \"Diffusion of Innovation\"[majr] OR \"Translational Research, Biomedical\"[majr] OR implement*[ti] OR delivery-science[ti] OR dissemination-science[ti] OR dissemination-research[ti] OR knowledge-translation[ti] OR translational-research[ti] OR innovation[ti] OR real-world[ti] OR conceptual-determinant*[ti] OR contextual[ti] OR facilitator*[ti] OR barriers[ti] OR enabler*[ti] OR program-evaluation[ti] OR process-evaluation[ti] OR \"Implement Sci\"[Journal] OR \"JBI Evid Implement\"[Journal] OR \"Transl Res\"[Journal] OR ((intervention*[ti] OR initiative*[ti] OR program[ti] OR programs[ti]) AND (acceptability[ti] OR actual-fit[ti] OR adopt*[ti] OR Appropriateness[ti] OR compatibility[ti] OR continuation[ti] OR disseminat*[ti] OR durability[ti] OR feasib*[ti] OR fidelity[ti] OR incorporat*[ti] OR institutionalization[ti] OR integration[ti] OR integrity[ti] OR intention-to-try[ti] OR maintenance[ti] OR optimiz*[ti] OR penetrat*[ti] OR perceived-fit[ti] OR practicability[ti] OR practicable[ti] OR reach[ti] OR relevance[ti] OR retention[ti] OR routiniz*[ti] OR routine-use[ti] OR Suitab*[ti] OR Sustainab*[ti] OR sustained-use[ti] OR uptake[ti] OR usefulness[ti] OR utility[ti] OR utilization[ti]))) \"\"\"\n",
    "\n",
    "#HIV AND implementation (i.e. HIV Implementation)\n",
    "base_query = \"\"\"(HIV[ti] OR HIV1[ti] OR HIVI[ti] OR HIVII[ti] OR HIV2[ti] OR human-immuno-deficiency-virus*[ti] OR human-immunodeficiency-virus*[ti] OR acquired-immune-deficiency-syndrome*[ti] OR acquired-immunodeficiency-syndrome*[ti] OR acquired-immuno-deficiency-syndrome*[ti] OR acute-retroviral-syndrome[ti] OR acute-seroconversion-syndrome[ti] OR Highly-Active-Antiretroviral-Therapy[ti] OR HAART[ti] OR PLWH[ti] OR WLWH[ti] OR MLWH[ti] OR ALWH[ti] OR PLHIV[ti] OR PLWHIV[ti] OR aids-related[ti] OR aids-infected[ti] OR aids-affected[ti] OR living-with-aids[ti] OR patients-with-aids[ti] OR people-with-aids[ti] OR women-with-aids[ti] OR men-with-aids[ti] OR aids-patients[ti] OR \"AIDS\"[Journal] OR \"AIDS Behav\"[Journal] OR \"AIDS Care\"[Journal] OR \"AIDS Educ Prev\"[Journal] OR \"AIDS Patient Care STDS\"[Journal] OR \"AIDS Res Hum Retroviruses\"[Journal] OR \"AIDS Res Ther\"[Journal] OR \"AIDS Rev\"[Journal] OR \"Curr HIV/AIDS Rep\"[Journal] OR \"Curr HIV Res\"[Journal] OR \"Curr Opin HIV AIDS\"[Journal] OR \"HIV Med\"[Journal] OR \"HIV Res Clin Pract\"[Journal] OR \"Int J STD AIDS\"[Journal] OR \"J Acquir Immune Defic Syndr\"[Journal] OR \"J Assoc Nurses AIDS Care\"[Journal] OR \"J Int AIDS Soc\"[Journal] OR \"Lancet HIV\"[Journal] OR \"J Int Assoc Provid AIDS Care\"[Journal] OR \"SAHARA J\"[Journal] OR (Aids[ti] NOT (decision-aid*[ti] OR decision-support-aid*[ti] OR decisionmaking-aid*[ti] OR decision-making-aid*[ti] OR hearing-aid*[ti] OR auditory-aid*[ti] OR audiovisual-aid*[ti] OR visual-aid*[ti] OR sensory-aid*[ti] OR communication-aid*[ti] OR cognitive-aid*[ti] OR mobility-aid*[ti] OR walking-aid*[ti] OR sleep-aid*[ti] OR vision-aid*[ti] OR cessation-aid*[ti] OR conversation-aid*[ti] OR memory-aid*[ti] OR diagnostic-aid*[ti] OR pharmacologic-aid*[ti] OR pharmaceutical-aid*[ti] OR medication-aid*[ti] OR job-aid*[ti] OR visualization-aid*[ti] OR pictorial-aid*[ti] OR training-aid*[ti] OR rehabilitation-aid*[ti]))) AND (english[lang] AND {DATE_FILTER} NOT ((africa[Mesh] OR \"caribbean region\"[Mesh] OR \"central america\"[Mesh] OR \"latin america\"[Mesh:NoExp] OR canada[Mesh] OR greenland[Mesh:NoExp] OR mexico[Mesh:NoExp] OR \"south america\"[Mesh] OR \"antarctic regions\"[Mesh:NoExp] OR \"arctic regions\"[Mesh:NoExp] OR asia[Mesh] OR europe[Mesh] OR oceania[Mesh] OR \"afghanistan\"[ti] OR \"africa\"[ti] OR \"albania\"[ti] OR \"algeria\"[ti] OR \"american samoa\"[ti] OR \"andorra\"[ti] OR \"angola\"[ti] OR \"anguilla\"[ti] OR \"antarctica\"[ti] OR \"antigua and barbuda\"[ti] OR \"argentina\"[ti] OR \"armenia\"[ti] OR \"aruba\"[ti] OR \"asia\"[ti] OR \"australia\"[ti] OR \"austria\"[ti] OR \"azerbaijan\"[ti] OR \"bahamas\"[ti] OR \"bahrain\"[ti] OR \"balkan states\"[ti] OR \"baltic states\"[ti] OR \"bangladesh\"[ti] OR \"barbados\"[ti] OR \"belarus\"[ti] OR \"belgium\"[ti] OR \"belize\"[ti] OR \"benin\"[ti] OR \"bermuda\"[ti] OR \"bhutan\"[ti] OR \"bolivia\"[ti] OR \"bosnia-herzegovina\"[ti] OR \"botswana\"[ti] OR \"bouvet\"[ti] OR \"brazil\"[ti] OR \"brunei\"[ti] OR \"bulgaria\"[ti] OR \"burkina faso\"[ti] OR \"burundi\"[ti] OR \"cambodia\"[ti] OR \"cameroon\"[ti] OR \"canada\"[ti] OR \"cape verde\"[ti] OR \"central african republic\"[ti] OR \"central america\"[ti] OR \"chad\"[ti] OR \"chile\"[ti] OR \"china\"[ti] OR \"colombia\"[ti] OR \"commonwealth of independent states\"[ti] OR \"comoros\"[ti] OR \"cook\"[ti] OR \"coral sea\"[ti] OR \"costa rica\"[ti] OR \"croatia\"[ti] OR \"cuba\"[ti] OR \"curacao\"[ti] OR \"cyprus\"[ti] OR \"czech republic\"[ti] OR \"czechoslovakia\"[ti] OR \"denmark\"[ti] OR \"djibouti\"[ti] OR \"dominica\"[ti] OR \"dominican republic\"[ti] OR \"east timor\"[ti] OR \"eastern europe\"[ti] OR \"ecuador\"[ti] OR \"egypt\"[ti] OR \"el salvador\"[ti] OR \"england\"[ti] OR \"equatorial guinea\"[ti] OR \"eritrea\"[ti] OR \"estonia\"[ti] OR \"ethiopia\"[ti] OR \"europa\"[ti] OR \"europe\"[ti] OR \"falkland\"[ti] OR \"faroe\"[ti] OR \"fiji\"[ti] OR \"finland\"[ti] OR \"france\"[ti] OR \"french guiana\"[ti] OR \"french polynesia\"[ti] OR \"gabon\"[ti] OR \"gambia\"[ti] OR \"gaza strip\"[ti] OR \"georgia\"[ti] OR \"germany\"[ti] OR \"ghana\"[ti] OR \"gibraltar\"[ti] OR \"great britain\"[ti] OR \"greece\"[ti] OR \"greenland\"[ti] OR \"grenada\"[ti] OR \"guadeloupe\"[ti] OR \"guam\"[ti] OR \"guatemala\"[ti] OR \"guernsey\"[ti] OR \"guinea\"[ti] OR \"guyana\"[ti] OR \"haiti\"[ti] OR \"honduras\"[ti] OR \"hong kong\"[ti] OR \"hungary\"[ti] OR \"iceland\"[ti] OR \"india\"[ti] OR \"indonesia\"[ti] OR \"iran\"[ti] OR \"iraq\"[ti] OR \"ireland\"[ti] OR \"israel\"[ti] OR \"italy\"[ti] OR \"ivory coast\"[ti] OR \"jamaica\"[ti] OR \"jan mayen\"[ti] OR \"japan\"[ti] OR \"jersey\"[ti] OR \"johnston atoll\"[ti] OR \"jordan\"[ti] OR \"juan de nova\"[ti] OR \"kazakhstan\"[ti] OR \"kenya\"[ti] OR \"kiribati\"[ti] OR \"korea\"[ti] OR \"kosovo\"[ti] OR \"kuwait\"[ti] OR \"kyrgyzstan\"[ti] OR \"laos\"[ti] OR \"latin america\"[ti] OR \"latvia\"[ti] OR \"lebanon\"[ti] OR \"lesotho\"[ti] OR \"liberia\"[ti] OR \"libya\"[ti] OR \"liechtenstein\"[ti] OR \"lithuania\"[ti] OR \"luxembourg\"[ti] OR \"macao\"[ti] OR \"macedonia\"[ti] OR \"madagascar\"[ti] OR \"malawi\"[ti] OR \"malaysia\"[ti] OR \"maldives\"[ti] OR \"mali\"[ti] OR \"malta\"[ti] OR \"marshall\"[ti] OR \"martinique\"[ti] OR \"mauritania\"[ti] OR \"mauritius\"[ti] OR \"mexico\"[ti] OR \"micronesia\"[ti] OR \"middle east\"[ti] OR \"midway\"[ti] OR \"moldova\"[ti] OR \"monaco\"[ti] OR \"mongolia\"[ti] OR \"montserrat\"[ti] OR \"morocco\"[ti] OR \"mozambique\"[ti] OR \"myanmar\"[ti] OR \"namibia\"[ti] OR \"nauru\"[ti] OR \"navassa\"[ti] OR \"nepal\"[ti] OR \"netherlands\"[ti] OR \"netherlands antilles\"[ti] OR \"new caledonia\"[ti] OR \"new zealand\"[ti] OR \"nicaragua\"[ti] OR \"niger\"[ti] OR \"nigeria\"[ti] OR \"niue\"[ti] OR \"norfolk\"[ti] OR \"north macedonia\"[ti] OR \"northern ireland\"[ti] OR \"northern mariana\"[ti] OR \"norway\"[ti] OR \"oman\"[ti] OR \"pakistan\"[ti] OR \"palau\"[ti] OR \"palestine\"[ti] OR \"panama\"[ti] OR \"papua new guinea\"[ti] OR \"paraguay\"[ti] OR \"peru\"[ti] OR \"philippines\"[ti] OR \"pitcairn\"[ti] OR \"poland\"[ti] OR \"portugal\"[ti] OR \"puerto rico\"[ti] OR \"qatar\"[ti] OR \"reunion\"[ti] OR \"romania\"[ti] OR \"russia\"[ti] OR \"rwanda\"[ti] OR \"saint helena\"[ti] OR \"saint kitts and nevis\"[ti] OR \"saint lucia\"[ti] OR \"saint pierre and miquelon\"[ti] OR \"saint vincent and the grenadines\"[ti] OR \"samoa\"[ti] OR \"san marino\"[ti] OR \"sao tome and principe\"[ti] OR \"saudi arabia\"[ti] OR \"senegal\"[ti] OR \"serbia\"[ti] OR \"seychelles\"[ti] OR \"sierra leone\"[ti] OR \"singapore\"[ti] OR \"slovakia\"[ti] OR \"slovenia\"[ti] OR \"solomon\"[ti] OR \"somalia\"[ti] OR \"south africa\"[ti] OR \"south america\"[ti] OR \"south korea\"[ti] OR \"spain\"[ti] OR \"sri lanka\"[ti] OR \"sudan\"[ti] OR \"suriname\"[ti] OR \"svalbard\"[ti] OR \"swaziland\"[ti] OR \"sweden\"[ti] OR \"switzerland\"[ti] OR \"syria\"[ti] OR \"taiwan\"[ti] OR \"tajikistan\"[ti] OR \"tanzania\"[ti] OR \"thailand\"[ti] OR \"togo\"[ti] OR \"tokelau\"[ti] OR \"tonga\"[ti] OR \"Trinidad\"[ti] OR \"tobago\"[ti] OR \"tunisia\"[ti] OR \"turkey\"[ti] OR \"turkmenistan\"[ti] OR \"tuvalu\"[ti] OR \"uganda\"[ti] OR \"ukraine\"[ti] OR \"united arab emirates\"[ti] OR \"united kingdom\"[ti] OR \"united states\"[ti] OR \"uruguay\"[ti] OR \"uzbekistan\"[ti] OR \"vanuatu\"[ti] OR \"vatican city\"[ti] OR \"venezuela\"[ti] OR \"vietnam\"[ti] OR \"virgin\"[ti] OR \"wallis and futuna\"[ti] OR \"west bank\"[ti] OR \"western sahara\"[ti] OR \"yemen\"[ti] OR \"yugoslavia\"[ti] OR \"zaire\"[ti] OR \"zambia\"[ti] OR \"zimbabwe\"[ti]) NOT (\"north america\"[Mesh:NoExp] OR \"united states\"[Mesh] OR \"north American people\"[Mesh:NoExp] OR \"American indian or alaska native\"[Mesh] OR \"population groups, us\"[Mesh:NoExp] OR north-america*[tiab] OR united-states[tiab] OR usa[tiab] OR us[tiab] OR \"u s a\"[tiab] OR \"u s\"[tiab] OR alabama[tiab] OR alaska[tiab] OR arizona[tiab] OR arkansas[tiab] OR california[tiab] OR colorado[tiab] OR connecticut[tiab] OR delaware[tiab] OR district-of-columbia[tiab] OR florida[tiab] OR georgia[tiab] OR hawaii[tiab] OR idaho[tiab] OR illinois[tiab] OR indiana[tiab] OR iowa[tiab] OR kansas[tiab] OR kentucky[tiab] OR louisiana[tiab] OR maine[tiab] OR maryland[tiab] OR massachusetts[tiab] OR michigan[tiab] OR minnesota[tiab] OR mississippi[tiab] OR missouri[tiab] OR montana[tiab] OR nebraska[tiab] OR nevada[tiab] OR new-hampshire[tiab] OR new-jersey[tiab] OR new-mexico[tiab] OR new-york[tiab] OR north-carolina[tiab] OR north-dakota[tiab] OR ohio[tiab] OR oklahoma[tiab] OR oregon[tiab] OR pennsylvania[tiab] OR rhode-island[tiab] OR south-carolina[tiab] OR south-dakota[tiab] OR tennessee[tiab] OR texas[tiab] OR utah[tiab] OR vermont[tiab] OR virginia[tiab] OR washington[tiab] OR west-virginia[tiab] OR wisconsin[tiab] OR wyoming[tiab]))) AND (\"Implementation Science\"[majr] OR \"Diffusion of Innovation\"[majr] OR \"Translational Research, Biomedical\"[majr] OR implement*[ti] OR delivery-science[ti] OR dissemination-science[ti] OR dissemination-research[ti] OR knowledge-translation[ti] OR translational-research[ti] OR innovation[ti] OR real-world[ti] OR conceptual-determinant*[ti] OR contextual[ti] OR facilitator*[ti] OR barriers[ti] OR enabler*[ti] OR program-evaluation[ti] OR process-evaluation[ti] OR \"Implement Sci\"[Journal] OR \"JBI Evid Implement\"[Journal] OR \"Transl Res\"[Journal] OR ((intervention*[ti] OR initiative*[ti] OR program[ti] OR programs[ti]) AND (acceptability[ti] OR actual-fit[ti] OR adopt*[ti] OR Appropriateness[ti] OR compatibility[ti] OR continuation[ti] OR disseminat*[ti] OR durability[ti] OR feasib*[ti] OR fidelity[ti] OR incorporat*[ti] OR institutionalization[ti] OR integration[ti] OR integrity[ti] OR intention-to-try[ti] OR maintenance[ti] OR optimiz*[ti] OR penetrat*[ti] OR perceived-fit[ti] OR practicability[ti] OR practicable[ti] OR reach[ti] OR relevance[ti] OR retention[ti] OR routiniz*[ti] OR routine-use[ti] OR Suitab*[ti] OR Sustainab*[ti] OR sustained-use[ti] OR uptake[ti] OR usefulness[ti] OR utility[ti] OR utilization[ti]))) \"\"\"\n",
    "\n",
    "# DATE RANGES\n",
    "date_ranges = [\n",
    "    (\"2000/01/01\", \"2000/06/30\"), (\"2000/07/01\", \"2000/12/31\"),\n",
    "    (\"2001/01/01\", \"2001/06/30\"), (\"2001/07/01\", \"2001/12/31\"),\n",
    "    (\"2002/01/01\", \"2002/06/30\"), (\"2002/07/01\", \"2002/12/31\"),\n",
    "    (\"2003/01/01\", \"2003/06/30\"), (\"2003/07/01\", \"2003/12/31\"),\n",
    "    (\"2004/01/01\", \"2004/06/30\"), (\"2004/07/01\", \"2004/12/31\"),\n",
    "    (\"2005/01/01\", \"2005/06/30\"), (\"2005/07/01\", \"2005/12/31\"),\n",
    "    (\"2006/01/01\", \"2006/06/30\"), (\"2006/07/01\", \"2006/12/31\"),\n",
    "    (\"2007/01/01\", \"2007/06/30\"), (\"2007/07/01\", \"2007/12/31\"),\n",
    "    (\"2008/01/01\", \"2008/06/30\"), (\"2008/07/01\", \"2008/12/31\"),\n",
    "    (\"2009/01/01\", \"2009/06/30\"), (\"2009/07/01\", \"2009/12/31\"),\n",
    "    (\"2010/01/01\", \"2010/06/30\"), (\"2010/07/01\", \"2010/12/31\"),\n",
    "    (\"2011/01/01\", \"2011/06/30\"), (\"2011/07/01\", \"2011/12/31\"),\n",
    "    (\"2012/01/01\", \"2012/06/30\"), (\"2012/07/01\", \"2012/12/31\"),\n",
    "    (\"2013/01/01\", \"2013/06/30\"), (\"2013/07/01\", \"2013/12/31\"),\n",
    "    (\"2014/01/01\", \"2014/06/30\"), (\"2014/07/01\", \"2014/12/31\"),\n",
    "    (\"2015/01/01\", \"2015/06/30\"), (\"2015/07/01\", \"2015/12/31\"),\n",
    "    (\"2016/01/01\", \"2016/06/30\"), (\"2016/07/01\", \"2016/12/31\"),\n",
    "    (\"2017/01/01\", \"2017/06/30\"), (\"2017/07/01\", \"2017/12/31\"),\n",
    "    (\"2018/01/01\", \"2018/06/30\"), (\"2018/07/01\", \"2018/12/31\"),\n",
    "    (\"2019/01/01\", \"2019/06/30\"), (\"2019/07/01\", \"2019/12/31\"),\n",
    "    (\"2020/01/01\", \"2020/06/30\"), (\"2020/07/01\", \"2020/12/31\"),\n",
    "    (\"2021/01/01\", \"2021/06/30\"), (\"2021/07/01\", \"2021/12/31\"),\n",
    "    (\"2022/01/01\", \"2022/06/30\"), (\"2022/07/01\", \"2022/12/31\"),\n",
    "    (\"2023/01/01\", \"2023/06/30\"), (\"2023/07/01\", \"2023/12/31\"),\n",
    "    (\"2024/01/01\", \"2024/06/30\"), (\"2024/07/01\", \"2024/12/31\"),\n",
    "    (\"2025/01/01\", \"2025/12/03\"),\n",
    "]\n",
    "\n",
    "affiliations_to_check = [\"Northwestern University\", \"Feinberg School of Medicine\"]\n",
    "\n",
    "# Store all chunk data\n",
    "all_chunk_files = []\n",
    "\n",
    "# ========================================\n",
    "#  MAIN LOOP\n",
    "# ========================================\n",
    "\n",
    "for chunk_num, (start_date, end_date) in enumerate(date_ranges, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CHUNK {chunk_num}/{len(date_ranges)}: {start_date} to {end_date}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Check if chunk file already exists (save to OUTPUT_FOLDER)\n",
    "    chunk_filename = os.path.join(OUTPUT_FOLDER, f'hiv_imp_chunk_{chunk_num:02d}_{start_date.replace(\"/\", \"-\")}_{end_date.replace(\"/\", \"-\")}.csv')\n",
    "    \n",
    "    if os.path.exists(chunk_filename):\n",
    "        print(f\"‚úì Chunk {chunk_num} already exists, skipping...\")\n",
    "        all_chunk_files.append(chunk_filename)\n",
    "        continue\n",
    "    \n",
    "    # Create query for this date range\n",
    "    date_filter = f\"{start_date}:{end_date}[pdat]\"\n",
    "    search_query = base_query.replace(\"{DATE_FILTER}\", date_filter)\n",
    "    \n",
    "    # Post search to history server\n",
    "    print(\"Posting search to NCBI history server...\")\n",
    "    try:\n",
    "        search_handle = Entrez.esearch(\n",
    "            db=\"pubmed\",\n",
    "            term=search_query,\n",
    "            usehistory=\"y\",\n",
    "            retmax=0\n",
    "        )\n",
    "        search_results = Entrez.read(search_handle)\n",
    "        search_handle.close()\n",
    "        \n",
    "        count = int(search_results[\"Count\"])\n",
    "        webenv = search_results[\"WebEnv\"]\n",
    "        query_key = search_results[\"QueryKey\"]\n",
    "        \n",
    "        print(f\"Total results: {count:,}\")\n",
    "        print(f\"WebEnv: {webenv[:20]}...\")\n",
    "        print(f\"QueryKey: {query_key}\")\n",
    "        logging.info(f\"Search posted to history server. Count: {count}, WebEnv: {webenv}, QueryKey: {query_key}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to post search: {e}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        count = 0\n",
    "    \n",
    "    # Check for existing checkpoint\n",
    "    checkpoint = load_latest_checkpoint()\n",
    "    \n",
    "    if checkpoint and checkpoint['total_count'] == count:\n",
    "        pubmed_data = checkpoint['pubmed_data']\n",
    "        failed_batches = checkpoint['failed_batches']\n",
    "        start_index = checkpoint['batch_index']\n",
    "        print(f\"\\nResuming from checkpoint: {len(pubmed_data):,} records already processed\")\n",
    "        print(f\"Starting from record {start_index:,}\")\n",
    "    else:\n",
    "        pubmed_data = []\n",
    "        failed_batches = []\n",
    "        start_index = 0\n",
    "    \n",
    "    if count > 0:\n",
    "        print(f\"\\nProcessing {count:,} records in batches of {BATCH_SIZE}...\")\n",
    "        \n",
    "        # Process records using history server\n",
    "        for start in tqdm(range(start_index, count, BATCH_SIZE), desc=\"Processing records\"):\n",
    "            try:\n",
    "                batch_records = fetch_records_from_history(webenv, query_key, start, BATCH_SIZE)\n",
    "                \n",
    "                if batch_records and 'PubmedArticle' in batch_records:\n",
    "                    for article in batch_records['PubmedArticle']:\n",
    "                        try:\n",
    "                            # Process each article\n",
    "                            processed = process_pubmed_record({'PubmedArticle': [article]}, affiliations_to_check)\n",
    "                            pubmed_data.append(processed)\n",
    "                        except Exception as e:\n",
    "                            try:\n",
    "                                pmid = article['MedlineCitation']['PMID']\n",
    "                            except:\n",
    "                                pmid = 'Unknown'\n",
    "                            logging.error(f\"Error processing PMID {pmid}: {e}\")\n",
    "                else:\n",
    "                    failed_batches.append((start, min(start + BATCH_SIZE, count)))\n",
    "                    logging.warning(f\"Batch at position {start} failed\")\n",
    "                \n",
    "                # Save checkpoint every 50 batches\n",
    "                batch_number = (start // BATCH_SIZE) + 1\n",
    "                if batch_number % CHECKPOINT_INTERVAL == 0:\n",
    "                    save_checkpoint(start + BATCH_SIZE, pubmed_data, failed_batches, count)\n",
    "                    print(f\"\\nCheckpoint saved at batch {batch_number} ({len(pubmed_data):,} records)\")\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(0.1 if Entrez.api_key else 0.34)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error at position {start}: {e}\")\n",
    "                failed_batches.append((start, min(start + BATCH_SIZE, count)))\n",
    "                time.sleep(2)\n",
    "        \n",
    "        # Save final checkpoint\n",
    "        save_checkpoint(count, pubmed_data, failed_batches, count)\n",
    "    \n",
    "        # Retry failed batches\n",
    "        if failed_batches:\n",
    "            print(f\"\\nRetrying {len(failed_batches)} failed batches...\")\n",
    "            for start, end in tqdm(failed_batches, desc=\"Retrying failed batches\"):\n",
    "                try:\n",
    "                    batch_records = fetch_records_from_history(webenv, query_key, start, end - start, max_retries=5)\n",
    "                    \n",
    "                    if batch_records and 'PubmedArticle' in batch_records:\n",
    "                        for article in batch_records['PubmedArticle']:\n",
    "                            try:\n",
    "                                processed = process_pubmed_record({'PubmedArticle': [article]}, affiliations_to_check)\n",
    "                                pubmed_data.append(processed)\n",
    "                            except Exception as e:\n",
    "                                try:\n",
    "                                    pmid = article['MedlineCitation']['PMID']\n",
    "                                except:\n",
    "                                    pmid = 'Unknown'\n",
    "                                logging.error(f\"Error processing PMID {pmid}: {e}\")\n",
    "                    \n",
    "                    time.sleep(0.5)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Failed retry at position {start}: {e}\")\n",
    "        \n",
    "        # Save this chunk's data (to OUTPUT_FOLDER)\n",
    "        if pubmed_data:\n",
    "            chunk_df = pd.DataFrame(pubmed_data)\n",
    "            chunk_df.to_csv(chunk_filename, index=False)\n",
    "            all_chunk_files.append(chunk_filename)\n",
    "            print(f\"\\n‚úì Chunk {chunk_num} complete: {len(chunk_df):,} records saved to {chunk_filename}\")\n",
    "            logging.info(f\"Chunk {chunk_num} saved: {len(chunk_df)} records\")\n",
    "        \n",
    "        # Brief pause between chunks\n",
    "        time.sleep(2)\n",
    "\n",
    "\n",
    "# ========================================\n",
    "#  COMBINE ALL CHUNKS\n",
    "# ========================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMBINING ALL CHUNKS\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "if all_chunk_files:\n",
    "    all_dfs = []\n",
    "    for filename in all_chunk_files:\n",
    "        df = pd.read_csv(filename)\n",
    "        all_dfs.append(df)\n",
    "        print(f\"Loaded {filename}: {len(df):,} records\")\n",
    "    \n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    combined_df = combined_df.drop_duplicates(subset='PMID', keep='first')\n",
    "    \n",
    "    # Save final output to OUTPUT_FOLDER\n",
    "    final_output = os.path.join(OUTPUT_FOLDER, 'hiv_imp_us_2000_2025_FINAL.csv')\n",
    "    combined_df.to_csv(final_output, index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"‚úì PROCESSING COMPLETE!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total chunks processed: {len(all_chunk_files)}\")\n",
    "    print(f\"Total unique records: {len(combined_df):,}\")\n",
    "    print(f\"Final output: {final_output}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    logging.info(f\"All {len(all_chunk_files)} chunks combined: {len(combined_df)} unique records\")\n",
    "else:\n",
    "    print(\"\\n‚ö† No data retrieved from any chunks!\")\n",
    "    logging.warning(\"No chunks produced data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eadd942-3ba2-418a-8259-ab04a8b99acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "#  Use spaCy to identify geolocations\n",
    "# ========================================\n",
    "\n",
    "# ========================================\n",
    "# CONFIGURATION - CHANGE THIS FOR EACH RUN\n",
    "# ========================================\n",
    "#INPUT_FOLDER = 'hiv imp'  # Change to 'hiv imp' for the other dataset\n",
    "INPUT_FOLDER = 'hiv not imp'  # Change to 'hiv imp' for the other dataset\n",
    "# ========================================\n",
    "\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_locations_spacy(text):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    doc = nlp(text)\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
    "    return list(set(locations)) if locations else None\n",
    "\n",
    "# Load file from folder\n",
    "input_file = os.path.join(INPUT_FOLDER, 'hiv_us_2000_2025_FINAL.csv')\n",
    "print(f\"Loading {input_file}...\")\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"ERROR: File not found: {input_file}\")\n",
    "    print(f\"Available files in {INPUT_FOLDER}:\")\n",
    "    if os.path.exists(INPUT_FOLDER):\n",
    "        for f in os.listdir(INPUT_FOLDER):\n",
    "            if f.endswith('.csv'):\n",
    "                print(f\"  {f}\")\n",
    "else:\n",
    "    combined_df = pd.read_csv(input_file)\n",
    "    print(f\"Loaded {len(combined_df):,} records\")\n",
    "    \n",
    "    # Prepare text\n",
    "    combined_df['TitleAbstract'] = combined_df['ArticleTitle'].fillna('') + ' ' + combined_df['Abstract'].fillna('')\n",
    "    \n",
    "    # Process in chunks with checkpoints\n",
    "    print(\"Extracting geographic locations...\")\n",
    "    chunk_size = 10000\n",
    "    all_locations = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(combined_df), chunk_size)):\n",
    "        chunk = combined_df.iloc[i:i+chunk_size]\n",
    "        chunk_locations = chunk['TitleAbstract'].apply(extract_locations_spacy)\n",
    "        all_locations.extend(chunk_locations.tolist())\n",
    "        \n",
    "        # Save checkpoint every 50k records\n",
    "        if (i + chunk_size) % 50000 == 0:\n",
    "            print(f\"\\nCheckpoint: processed {i + chunk_size:,} records\")\n",
    "    \n",
    "    combined_df['GeographicLocations'] = all_locations\n",
    "    \n",
    "    # Save final output to same folder\n",
    "    output_file = os.path.join(INPUT_FOLDER, 'hiv_us_2000_2025_with_locations.csv')\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úì Complete! Saved to {output_file}\")\n",
    "    print(f\"Total records with locations: {combined_df['GeographicLocations'].notna().sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69eeb9da-a6db-4ee7-86f7-4f0423ba8c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hiv_imp_us_2000_2025_FINAL.csv...\n",
      "Loaded 160,602 records\n",
      "Extracting geographic locations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                        | 5/17 [40:59<1:40:19, 501.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint: processed 50,000 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                               | 10/17 [1:28:20<1:04:50, 555.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint: processed 100,000 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã         | 15/17 [2:18:36<20:15, 607.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checkpoint: processed 150,000 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [2:30:23<00:00, 530.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Complete! Saved to hiv not imp/hiv_us_2000_2025_with_locations.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc7c72e4-968a-459e-b58a-3b8248d4c1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hiv not imp\\hiv_us_2000_2025_with_locations.csv...\n",
      "Loaded 160,602 records\n",
      "Extracting unique locations...\n",
      "\n",
      "‚úì Exported 11173 unique locations to hiv not imp\\hiv_locations_to_review.xlsx\n",
      "Total location mentions: 90,154\n",
      "\n",
      "Top 20 most common locations:\n",
      "  the United States: 5,778\n",
      "  US: 3,665\n",
      "  U.S.: 1,893\n",
      "  USA: 1,594\n",
      "  RT: 1,351\n",
      "  South Africa: 1,306\n",
      "  New York City: 1,104\n",
      "  PWH: 968\n",
      "  syphilis: 937\n",
      "  CCR5: 890\n",
      "  PLWH: 872\n",
      "  United States: 846\n",
      "  DC: 822\n",
      "  CD8: 801\n",
      "  MD: 710\n",
      "  San Francisco: 706\n",
      "  India: 675\n",
      "  NRTIs: 643\n",
      "  China: 619\n",
      "  California: 612\n",
      "\n",
      "Next steps:\n",
      "1. Open hiv not imp\\hiv_locations_to_review.xlsx in Excel\n",
      "2. Put 'X' in the DELETE? column for false positives\n",
      "3. Save the file\n",
      "4. Run Step 2 code to create exclusion list\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "#  Exract Unique Locations for Manual Review\n",
    "# ========================================\n",
    "\n",
    "# ========================================\n",
    "# CONFIGURATION - CHANGE THIS FOR EACH RUN\n",
    "# ========================================\n",
    "#INPUT_FOLDER = 'hiv imp'  # Change to 'hiv not imp' for the other dataset\n",
    "INPUT_FOLDER = 'hiv not imp'  \n",
    "# ========================================\n",
    "\n",
    "# Determine input filename based on folder\n",
    "if INPUT_FOLDER == 'hiv imp':\n",
    "    input_filename = 'hiv_imp_us_2000_2025_with_locations.csv'\n",
    "    output_filename = 'hiv_imp_locations_to_review.xlsx'\n",
    "else:  # 'hiv not imp'\n",
    "    input_filename = 'hiv_us_2000_2025_with_locations.csv'\n",
    "    output_filename = 'hiv_locations_to_review.xlsx'\n",
    "\n",
    "# Load file from folder\n",
    "input_file = os.path.join(INPUT_FOLDER, input_filename)\n",
    "print(f\"Loading {input_file}...\")\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"ERROR: File not found: {input_file}\")\n",
    "    print(f\"Available files in {INPUT_FOLDER}:\")\n",
    "    if os.path.exists(INPUT_FOLDER):\n",
    "        for f in os.listdir(INPUT_FOLDER):\n",
    "            if f.endswith('.csv'):\n",
    "                print(f\"  {f}\")\n",
    "else:\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"Loaded {len(df):,} records\")\n",
    "    \n",
    "    # Extract all locations\n",
    "    print(\"Extracting unique locations...\")\n",
    "    all_locations = []\n",
    "    for loc_str in df['GeographicLocations'].dropna():\n",
    "        try:\n",
    "            if isinstance(loc_str, str):\n",
    "                locs = ast.literal_eval(loc_str)\n",
    "                if isinstance(locs, list):\n",
    "                    all_locations.extend(locs)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Count frequencies\n",
    "    location_counts = Counter(all_locations)\n",
    "    \n",
    "    # Create DataFrame sorted alphabetically (easier to scan)\n",
    "    freq_df = pd.DataFrame([\n",
    "        {'Location': loc, 'Count': count} \n",
    "        for loc, count in sorted(location_counts.items())\n",
    "    ])\n",
    "    \n",
    "    # Add DELETE column for marking\n",
    "    freq_df['DELETE?'] = ''\n",
    "    \n",
    "    # Save to Excel in same folder\n",
    "    output_file = os.path.join(INPUT_FOLDER, output_filename)\n",
    "    freq_df.to_excel(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úì Exported {len(freq_df)} unique locations to {output_file}\")\n",
    "    print(f\"Total location mentions: {sum(location_counts.values()):,}\")\n",
    "    \n",
    "    print(\"\\nTop 20 most common locations:\")\n",
    "    for loc, count in location_counts.most_common(20):\n",
    "        print(f\"  {loc}: {count:,}\")\n",
    "    \n",
    "    print(\"\\nNext steps:\")\n",
    "    print(f\"1. Open {output_file} in Excel\")\n",
    "    print(\"2. Put 'X' in the DELETE? column for false positives\")\n",
    "    print(\"3. Save the file\")\n",
    "    print(\"4. Run Step 2 code to create exclusion list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b8ecb82-034b-4151-823b-1ac22a4518a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading hiv not imp\\hiv_locations_to_review_COMPLETED.xlsx...\n",
      "\n",
      "\n",
      "Total to remove: 44,226 location mentions\n",
      "\n",
      "Loading hiv not imp\\hiv_us_2000_2025_with_locations.csv...\n",
      "Loaded 160,602 records\n",
      "Cleaning locations...\n",
      "\n",
      "‚úì Saved cleaned file to hiv not imp\\hiv_us_2000_2025_with_locations_CLEANED.csv\n",
      "Removed 8969 false positive location types\n",
      "Total location mentions removed: 44,226\n",
      "\n",
      "Records with locations:\n",
      "  Before cleaning: 55,993\n",
      "  After cleaning: 28,316\n",
      "  Difference: 27,677\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "#  Remove Bad Locations from original file\n",
    "# ========================================\n",
    "\n",
    "# ========================================\n",
    "# CONFIGURATION - CHANGE THIS FOR EACH RUN\n",
    "# ========================================\n",
    "#INPUT_FOLDER = 'hiv imp'  # Change to 'hiv imp' for the other dataset\n",
    "INPUT_FOLDER = 'hiv not imp'  # Change to 'hiv imp' for the other dataset\n",
    "\n",
    "# ========================================\n",
    "\n",
    "# Determine filenames based on folder\n",
    "if INPUT_FOLDER == 'hiv imp':\n",
    "    review_filename = 'hiv_imp_locations_to_review_COMPLETED.xlsx'\n",
    "    input_filename = 'hiv_imp_us_2000_2025_with_locations.csv'\n",
    "    output_filename = 'hiv_imp_us_2000_2025_with_locations_CLEANED.csv'\n",
    "else:  # 'hiv not imp'\n",
    "    review_filename = 'hiv_locations_to_review_COMPLETED.xlsx'\n",
    "    input_filename = 'hiv_us_2000_2025_with_locations.csv'\n",
    "    output_filename = 'hiv_us_2000_2025_with_locations_CLEANED.csv'\n",
    "\n",
    "# ===== STEP 1: Read marked deletions from Excel =====\n",
    "review_file = os.path.join(INPUT_FOLDER, review_filename)\n",
    "print(f\"Reading {review_file}...\")\n",
    "\n",
    "if not os.path.exists(review_file):\n",
    "    print(f\"ERROR: Review file not found: {review_file}\")\n",
    "    print(f\"Make sure you've completed the review and saved it as '{review_filename}'\")\n",
    "else:\n",
    "    review_df = pd.read_excel(review_file)\n",
    "    \n",
    "    # Get locations marked for deletion\n",
    "    marked_for_deletion = review_df[review_df['DELETE?'].str.upper() == 'X']\n",
    "    \n",
    "    # print(f\"Found {len(marked_for_deletion)} locations marked for deletion\\n\")\n",
    "    # print(\"=\"*60)\n",
    "    # print(\"Copy this list into your cleaning code:\")\n",
    "    # print(\"=\"*60)\n",
    "    # print(\"\\nbad_locations = [\")\n",
    "    # for loc in marked_for_deletion['Location']:\n",
    "    #     print(f\"    '{loc}',\")\n",
    "    # print(\"]\")\n",
    "    print(f\"\\n\\nTotal to remove: {marked_for_deletion['Count'].sum():,} location mentions\")\n",
    "    \n",
    "    # ===== STEP 2: Create bad_locations list from marked items =====\n",
    "    bad_locations = marked_for_deletion['Location'].tolist()\n",
    "   # print(f\"\\nCreated bad_locations list with {len(bad_locations)} items\")\n",
    "    \n",
    "    # ===== STEP 3: Load and clean data =====\n",
    "    input_file = os.path.join(INPUT_FOLDER, input_filename)\n",
    "    print(f\"\\nLoading {input_file}...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    print(f\"Loaded {len(df):,} records\")\n",
    "    \n",
    "    def clean_locations(loc_list_str):\n",
    "        if pd.isna(loc_list_str):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Convert string to list\n",
    "            if isinstance(loc_list_str, str):\n",
    "                locs = ast.literal_eval(loc_list_str)\n",
    "            else:\n",
    "                locs = loc_list_str\n",
    "            \n",
    "            # Filter out bad locations\n",
    "            if isinstance(locs, list):\n",
    "                cleaned = [loc for loc in locs if loc not in bad_locations]\n",
    "                return cleaned if cleaned else None\n",
    "            else:\n",
    "                return None\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # Apply cleaning\n",
    "    print(\"Cleaning locations...\")\n",
    "    df['GeographicLocations'] = df['GeographicLocations'].apply(clean_locations)\n",
    "    \n",
    "    # Save cleaned version to same folder\n",
    "    output_file = os.path.join(INPUT_FOLDER, output_filename)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úì Saved cleaned file to {output_file}\")\n",
    "    print(f\"Removed {len(bad_locations)} false positive location types\")\n",
    "    print(f\"Total location mentions removed: {marked_for_deletion['Count'].sum():,}\")\n",
    "    \n",
    "    # Show before/after stats\n",
    "    original_with_locs = pd.read_csv(input_file)['GeographicLocations'].notna().sum()\n",
    "    cleaned_with_locs = df['GeographicLocations'].notna().sum()\n",
    "    print(f\"\\nRecords with locations:\")\n",
    "    print(f\"  Before cleaning: {original_with_locs:,}\")\n",
    "    print(f\"  After cleaning: {cleaned_with_locs:,}\")\n",
    "    print(f\"  Difference: {original_with_locs - cleaned_with_locs:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e66520f0-2ba7-4c2f-a045-b13e60ebf1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading hiv not imp\\hiv_us_2000_2025_with_locations_CLEANED.csv...\n",
      "Loaded 160,602 records\n",
      "Starting with empty cache\n",
      "\n",
      "Extracting unique locations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning records: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28316/28316 [00:00<00:00, 38409.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 2339 unique locations to classify\n",
      "\n",
      "--- PHASE 1: Rule-based classification ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rule-based detection: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2339/2339 [00:00<00:00, 40637.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Classified 484 locations as US using rules\n",
      "  Remaining unknowns: 1855\n",
      "\n",
      "--- PHASE 2: Geopy lookup for unknowns ---\n",
      "This will take approximately 34.0 minutes\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Proceed with geopy lookups? (y/n):  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Geopy lookups: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1855/1855 [49:33<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Cached 2339 location lookups\n",
      "\n",
      "--- PHASE 3: Classifying all records ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying records: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 160602/160602 [00:00<00:00, 170877.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASSIFICATION RESULTS\n",
      "============================================================\n",
      "Location_Detail\n",
      "No locations           132286\n",
      "US only                 16250\n",
      "Non-US only              9695\n",
      "Mixed (US + Non-US)      2371\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total records: 160,602\n",
      "\n",
      "‚úì Saved to hiv not imp\\hiv_us_2000_2025_with_location_classification.csv\n",
      "‚úì Saved location cache (2339 entries)\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "#  Flag geolocations as US or International\n",
    "# ========================================\n",
    "\n",
    "# ========================================\n",
    "# CONFIGURATION - CHANGE THIS FOR EACH RUN\n",
    "# ========================================\n",
    "#INPUT_FOLDER = 'hiv imp'  # Change to 'hiv imp' for the other dataset\n",
    "INPUT_FOLDER = 'hiv not imp'  # Change to 'hiv imp' for the other dataset\n",
    "# ========================================\n",
    "\n",
    "# Determine filenames based on folder\n",
    "if INPUT_FOLDER == 'hiv imp':\n",
    "    input_filename = 'hiv_imp_us_2000_2025_with_locations_CLEANED.csv'\n",
    "    output_filename = 'hiv_imp_us_2000_2025_with_location_classification.csv'\n",
    "    cache_filename = 'location_hiv_imp_cache.pkl'\n",
    "else:  # 'hiv not imp'\n",
    "    input_filename = 'hiv_us_2000_2025_with_locations_CLEANED.csv'\n",
    "    output_filename = 'hiv_us_2000_2025_with_location_classification.csv'\n",
    "    cache_filename = 'location_hiv_cache.pkl'\n",
    "\n",
    "# Load cleaned data\n",
    "input_file = os.path.join(INPUT_FOLDER, input_filename)\n",
    "print(f\"Loading {input_file}...\")\n",
    "\n",
    "if not os.path.exists(input_file):\n",
    "    print(f\"ERROR: File not found: {input_file}\")\n",
    "    print(f\"Available files in {INPUT_FOLDER}:\")\n",
    "    if os.path.exists(INPUT_FOLDER):\n",
    "        for f in os.listdir(INPUT_FOLDER):\n",
    "            if f.endswith('.csv'):\n",
    "                print(f\"  {f}\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "print(f\"Loaded {len(df):,} records\")\n",
    "\n",
    "# ===== STEP 1: Define rule-based US locations =====\n",
    "us_locations = {\n",
    "    # Country names\n",
    "    'United States', 'USA', 'US', 'U.S.', 'U.S.A.', 'America', 'United States of America',\n",
    "    \n",
    "    # States (full names)\n",
    "    'Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado',\n",
    "    'Connecticut', 'Delaware', 'Florida', 'Georgia', 'Hawaii', 'Idaho',\n",
    "    'Illinois', 'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana',\n",
    "    'Maine', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota',\n",
    "    'Mississippi', 'Missouri', 'Montana', 'Nebraska', 'Nevada',\n",
    "    'New Hampshire', 'New Jersey', 'New Mexico', 'New York',\n",
    "    'North Carolina', 'North Dakota', 'Ohio', 'Oklahoma', 'Oregon',\n",
    "    'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota',\n",
    "    'Tennessee', 'Texas', 'Utah', 'Vermont', 'Virginia', 'Washington',\n",
    "    'West Virginia', 'Wisconsin', 'Wyoming',\n",
    "    \n",
    "    # State abbreviations\n",
    "    'AL', 'AK', 'AZ', 'AR', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'ID',\n",
    "    'IL', 'IN', 'IA', 'KS', 'KY', 'LA', 'ME', 'MD', 'MA', 'MI', 'MN', 'MS',\n",
    "    'MO', 'MT', 'NE', 'NV', 'NH', 'NJ', 'NM', 'NY', 'NC', 'ND', 'OH', 'OK',\n",
    "    'OR', 'PA', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VT', 'VA', 'WA', 'WV',\n",
    "    'WI', 'WY',\n",
    "    \n",
    "    # Territories\n",
    "    'Puerto Rico', 'Guam', 'Virgin Islands', 'American Samoa',\n",
    "    'Northern Mariana Islands', 'District of Columbia',\n",
    "    \n",
    "    # Major cities\n",
    "    'New York City', 'NYC', 'Los Angeles', 'LA', 'Chicago', 'Houston', \n",
    "    'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', \n",
    "    'San Jose', 'Austin', 'Jacksonville', 'Fort Worth', 'Columbus', \n",
    "    'San Francisco', 'Charlotte', 'Indianapolis', 'Seattle', 'Denver', \n",
    "    'Washington DC', 'DC', 'Boston', 'Nashville', 'Detroit', 'Portland', \n",
    "    'Memphis', 'Atlanta', 'Miami', 'Baltimore', 'Minneapolis', 'Cleveland', \n",
    "    'New Orleans', 'Tampa', 'Pittsburgh', 'Cincinnati', 'Newark',\n",
    "    \n",
    "    # Metro variations\n",
    "    'Metropolitan DC', 'Metro DC', 'Washington Metropolitan Area',\n",
    "    'Greater Los Angeles', 'Greater New York', 'Bay Area', 'San Francisco Bay Area',\n",
    "    \n",
    "    # Regions\n",
    "    'North America', 'Midwest', 'Northeast', 'Southeast', 'Southwest',\n",
    "    'Pacific Northwest', 'New England', 'Mid-Atlantic', 'Deep South',\n",
    "}\n",
    "\n",
    "us_locations_lower = {loc.lower() for loc in us_locations}\n",
    "\n",
    "# Patterns indicating US locations\n",
    "us_patterns = [\n",
    "    r'\\bCounty\\b',\n",
    "    r'\\bParish\\b',\n",
    "    r'\\bBorough\\b',\n",
    "    r'\\bMetropolitan\\b',\n",
    "    r'\\bMetro\\b',\n",
    "    r'\\bGreater\\b',\n",
    "]\n",
    "\n",
    "# ===== STEP 2: Load or create cache =====\n",
    "cache_file = os.path.join(INPUT_FOLDER, cache_filename)\n",
    "if os.path.exists(cache_file):\n",
    "    with open(cache_file, 'rb') as f:\n",
    "        location_cache = pickle.load(f)\n",
    "    print(f\"Loaded {len(location_cache)} cached location lookups\")\n",
    "else:\n",
    "    location_cache = {}\n",
    "    print(\"Starting with empty cache\")\n",
    "\n",
    "# Initialize geocoder\n",
    "geolocator = Nominatim(user_agent=\"hiv_research_classifier_v1\")\n",
    "\n",
    "# ===== STEP 3: Hybrid detection function =====\n",
    "def is_us_location_hybrid(location, use_geopy=True):\n",
    "    \"\"\"\n",
    "    Three-stage detection:\n",
    "    1. Rule-based (instant)\n",
    "    2. Pattern matching (instant)\n",
    "    3. Geopy lookup (slow, only for unknowns)\n",
    "    \"\"\"\n",
    "    loc_lower = location.lower()\n",
    "    \n",
    "    # Stage 1: Direct match\n",
    "    if loc_lower in us_locations_lower:\n",
    "        return True\n",
    "    \n",
    "    # Stage 2: Pattern matching\n",
    "    for pattern in us_patterns:\n",
    "        if re.search(pattern, location, re.IGNORECASE):\n",
    "            return True\n",
    "    \n",
    "    # Check if state name is part of the location string\n",
    "    for state in us_locations:\n",
    "        if state in location and len(state) > 3:  # Avoid matching short abbreviations randomly\n",
    "            return True\n",
    "    \n",
    "    # Stage 3: Geopy (only for uncertain cases)\n",
    "    if use_geopy:\n",
    "        if location not in location_cache:\n",
    "            try:\n",
    "                time.sleep(1.1)  # Rate limit: 1 request per second\n",
    "                result = geolocator.geocode(location, addressdetails=True, timeout=10)\n",
    "                \n",
    "                if result and hasattr(result, 'raw'):\n",
    "                    country_code = result.raw.get('address', {}).get('country_code', '').upper()\n",
    "                    location_cache[location] = (country_code == 'US')\n",
    "                else:\n",
    "                    location_cache[location] = False\n",
    "                    \n",
    "            except Exception as e:\n",
    "                location_cache[location] = False\n",
    "        \n",
    "        return location_cache.get(location, False)\n",
    "    \n",
    "    return False\n",
    "\n",
    "# ===== STEP 4: Extract all unique locations =====\n",
    "print(\"\\nExtracting unique locations...\")\n",
    "all_locations = set()\n",
    "for loc_str in tqdm(df['GeographicLocations'].dropna(), desc=\"Scanning records\"):\n",
    "    try:\n",
    "        if isinstance(loc_str, str):\n",
    "            locs = ast.literal_eval(loc_str)\n",
    "            if isinstance(locs, list):\n",
    "                all_locations.update(locs)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"\\nFound {len(all_locations)} unique locations to classify\")\n",
    "\n",
    "# ===== STEP 5: First pass - rule-based only =====\n",
    "print(\"\\n--- PHASE 1: Rule-based classification ---\")\n",
    "unknown_locations = []\n",
    "\n",
    "for loc in tqdm(all_locations, desc=\"Rule-based detection\"):\n",
    "    is_us = is_us_location_hybrid(loc, use_geopy=False)\n",
    "    if is_us:\n",
    "        location_cache[loc] = True\n",
    "    else:\n",
    "        unknown_locations.append(loc)\n",
    "\n",
    "print(f\"‚úì Classified {len(all_locations) - len(unknown_locations)} locations as US using rules\")\n",
    "print(f\"  Remaining unknowns: {len(unknown_locations)}\")\n",
    "\n",
    "# ===== STEP 6: Second pass - geopy for unknowns =====\n",
    "if unknown_locations:\n",
    "    print(\"\\n--- PHASE 2: Geopy lookup for unknowns ---\")\n",
    "    print(f\"This will take approximately {len(unknown_locations) * 1.1 / 60:.1f} minutes\")\n",
    "    \n",
    "    user_input = input(\"\\nProceed with geopy lookups? (y/n): \").lower()\n",
    "    \n",
    "    if user_input == 'y':\n",
    "        for loc in tqdm(unknown_locations, desc=\"Geopy lookups\"):\n",
    "            is_us_location_hybrid(loc, use_geopy=True)\n",
    "        \n",
    "        # Save cache after geopy phase\n",
    "        with open(cache_file, 'wb') as f:\n",
    "            pickle.dump(location_cache, f)\n",
    "        print(f\"\\n‚úì Cached {len(location_cache)} location lookups\")\n",
    "    else:\n",
    "        print(\"\\nSkipping geopy lookups. Unknowns will be classified as Non-US.\")\n",
    "\n",
    "# ===== STEP 7: Classify all records =====\n",
    "print(\"\\n--- PHASE 3: Classifying all records ---\")\n",
    "\n",
    "def classify_location_detailed(loc_list_str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - 'US only'\n",
    "    - 'Mixed (US + Non-US)'\n",
    "    - 'Non-US only'\n",
    "    - 'No locations'\n",
    "    \"\"\"\n",
    "    if pd.isna(loc_list_str):\n",
    "        return 'No locations'\n",
    "    \n",
    "    try:\n",
    "        if isinstance(loc_list_str, str):\n",
    "            locs = ast.literal_eval(loc_list_str)\n",
    "        else:\n",
    "            locs = loc_list_str\n",
    "        \n",
    "        if not locs or not isinstance(locs, list):\n",
    "            return 'No locations'\n",
    "        \n",
    "        has_us = any(location_cache.get(loc, False) for loc in locs)\n",
    "        has_non_us = any(not location_cache.get(loc, False) for loc in locs)\n",
    "        \n",
    "        if has_us and has_non_us:\n",
    "            return 'Mixed (US + Non-US)'\n",
    "        elif has_us:\n",
    "            return 'US only'\n",
    "        elif has_non_us:\n",
    "            return 'Non-US only'\n",
    "        else:\n",
    "            return 'No locations'\n",
    "            \n",
    "    except:\n",
    "        return 'No locations'\n",
    "\n",
    "# Apply classification with progress bar\n",
    "tqdm.pandas(desc=\"Classifying records\")\n",
    "df['Location_Detail'] = df['GeographicLocations'].progress_apply(classify_location_detailed)\n",
    "\n",
    "# ===== STEP 8: Results and save =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(df['Location_Detail'].value_counts())\n",
    "print(f\"\\nTotal records: {len(df):,}\")\n",
    "\n",
    "# Save to same folder\n",
    "output_file = os.path.join(INPUT_FOLDER, output_filename)\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\n‚úì Saved to {output_file}\")\n",
    "\n",
    "# Final cache save\n",
    "with open(cache_file, 'wb') as f:\n",
    "    pickle.dump(location_cache, f)\n",
    "print(f\"‚úì Saved location cache ({len(location_cache)} entries)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc986c49-ec64-4d40-8dd1-1fb81093dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "#  Find additional implementation science papers using machine learning\n",
    "# ========================================\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ========================================\n",
    "# CONFIGURATION\n",
    "# ========================================\n",
    "OUTPUT_FOLDER = 'hiv imp proj validation'  # All outputs go here\n",
    "FILE_A_FOLDER = 'hiv not imp'  # Non-implementation data\n",
    "FILE_B_FOLDER = 'hiv imp'  # Implementation data\n",
    "# ========================================\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Define folders\n",
    "folder_a = FILE_A_FOLDER\n",
    "folder_b = FILE_B_FOLDER\n",
    "\n",
    "# Load both datasets\n",
    "file_a_path = os.path.join(folder_a, 'hiv_us_2000_2025_FINAL.csv')\n",
    "file_b_path = os.path.join(folder_b, 'hiv_imp_us_2000_2025_FINAL.csv')\n",
    "\n",
    "print(f\"Loading File A: {file_a_path}\")\n",
    "file_a = pd.read_csv(file_a_path)\n",
    "\n",
    "print(f\"Loading File B: {file_b_path}\")\n",
    "file_b = pd.read_csv(file_b_path)\n",
    "\n",
    "# Combine title + abstract for both\n",
    "file_a['text'] = file_a['ArticleTitle'].fillna('') + ' ' + file_a['Abstract'].fillna('')\n",
    "file_b['text'] = file_b['ArticleTitle'].fillna('') + ' ' + file_b['Abstract'].fillna('')\n",
    "\n",
    "# Remove empty texts\n",
    "file_a = file_a[file_a['text'].str.strip() != '']\n",
    "file_b = file_b[file_b['text'].str.strip() != '']\n",
    "\n",
    "print(f\"\\nFile A (non-implementation): {len(file_a):,} records\")\n",
    "print(f\"File B (implementation): {len(file_b):,} records\")\n",
    "\n",
    "# Create TF-IDF vectorizer on implementation papers\n",
    "print(\"\\nVectorizing implementation science papers...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=5,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Fit on File B (known implementation papers)\n",
    "tfidf_implementation = vectorizer.fit_transform(file_b['text'])\n",
    "\n",
    "# Get top distinguishing terms for implementation science\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_means = tfidf_implementation.mean(axis=0).A1\n",
    "top_terms_idx = tfidf_means.argsort()[-100:][::-1]\n",
    "\n",
    "print(\"\\nTop 50 implementation science terms/phrases:\")\n",
    "for idx in top_terms_idx[:50]:\n",
    "    print(f\"  {feature_names[idx]}\")\n",
    "\n",
    "# Save top terms to output folder\n",
    "top_terms_df = pd.DataFrame({\n",
    "    'Term': [feature_names[idx] for idx in top_terms_idx],\n",
    "    'TF-IDF Score': [tfidf_means[idx] for idx in top_terms_idx]\n",
    "})\n",
    "top_terms_file = os.path.join(OUTPUT_FOLDER, 'top_implementation_terms.csv')\n",
    "top_terms_df.to_csv(top_terms_file, index=False)\n",
    "print(f\"\\n‚úì Saved top terms to {top_terms_file}\")\n",
    "\n",
    "# ===== STAGE 2: Score File A papers =====\n",
    "print(\"\\nScoring File A papers...\")\n",
    "tfidf_file_a = vectorizer.transform(file_a['text'])\n",
    "\n",
    "# Calculate similarity to implementation science centroid\n",
    "implementation_centroid = tfidf_implementation.mean(axis=0)\n",
    "implementation_centroid = np.asarray(implementation_centroid)\n",
    "\n",
    "similarities = cosine_similarity(tfidf_file_a, implementation_centroid).flatten()\n",
    "\n",
    "# Add similarity scores to File A\n",
    "file_a['implementation_similarity'] = similarities\n",
    "\n",
    "# Sort by similarity\n",
    "file_a_sorted = file_a.sort_values('implementation_similarity', ascending=False)\n",
    "\n",
    "# Show distribution\n",
    "print(\"\\nSimilarity Score Distribution:\")\n",
    "print(file_a_sorted['implementation_similarity'].describe())\n",
    "\n",
    "# Export top candidates for review\n",
    "top_n = 1000\n",
    "top_candidates = file_a_sorted.head(top_n)[['PMID', 'ArticleTitle', 'Abstract', 'implementation_similarity']]\n",
    "top_candidates_file = os.path.join(OUTPUT_FOLDER, 'potential_implementation_papers.xlsx')\n",
    "top_candidates.to_excel(top_candidates_file, index=False)\n",
    "print(f\"\\n‚úì Exported top {top_n} candidates to {top_candidates_file}\")\n",
    "\n",
    "# ===== STAGE 3: Add concept flags =====\n",
    "print(\"\\nAdding concept flags...\")\n",
    "implementation_concepts = {\n",
    "    'scale_up': ['scale up', 'scale-up', 'scaling', 'scaleup'],\n",
    "    'real_world': ['real world', 'real-world', 'pragmatic', 'effectiveness'],\n",
    "    'pilot': ['pilot', 'demonstration', 'proof of concept'],\n",
    "    'rollout': ['rollout', 'roll out', 'deployment'],\n",
    "    'workflow': ['workflow', 'work flow', 'clinical workflow'],\n",
    "    'training': ['training program', 'capacity building', 'provider training'],\n",
    "    'guideline': ['guideline adherence', 'protocol adherence', 'clinical guidelines'],\n",
    "    'quality_improvement': ['quality improvement', 'QI', 'continuous improvement'],\n",
    "    'clinic_level': ['clinic level', 'clinic-level', 'facility level'],\n",
    "    'patient_centered': ['patient centered', 'patient-centered', 'patient engagement']\n",
    "}\n",
    "\n",
    "# Flag papers with these concepts\n",
    "for concept, terms in implementation_concepts.items():\n",
    "    pattern = '|'.join(terms)\n",
    "    file_a_sorted[f'has_{concept}'] = file_a_sorted['text'].str.contains(pattern, case=False, na=False)\n",
    "\n",
    "# Create composite flag\n",
    "concept_columns = [f'has_{concept}' for concept in implementation_concepts.keys()]\n",
    "file_a_sorted['num_concepts'] = file_a_sorted[concept_columns].sum(axis=1)\n",
    "\n",
    "print(\"‚úì Concept flags added\")\n",
    "\n",
    "# ===== STAGE 4: Stratified sampling for validation =====\n",
    "print(\"\\nCreating validation sample...\")\n",
    "bins = [0, 0.1, 0.2, 0.3, 0.4, 1.0]\n",
    "labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "file_a_sorted['similarity_bin'] = pd.cut(file_a_sorted['implementation_similarity'], bins=bins, labels=labels)\n",
    "\n",
    "samples = []\n",
    "for bin_label in labels:\n",
    "    bin_data = file_a_sorted[file_a_sorted['similarity_bin'] == bin_label]\n",
    "    sample = bin_data.sample(min(50, len(bin_data)), random_state=42)\n",
    "    samples.append(sample)\n",
    "\n",
    "validation_set = pd.concat(samples)\n",
    "validation_set['YOUR_REVIEW'] = ''\n",
    "\n",
    "validation_file = os.path.join(OUTPUT_FOLDER, 'validation_sample.xlsx')\n",
    "validation_set[['PMID', 'ArticleTitle', 'Abstract', 'implementation_similarity', \n",
    "                'num_concepts', 'similarity_bin', 'YOUR_REVIEW']].to_excel(validation_file, index=False)\n",
    "\n",
    "print(f\"\\n‚úì Exported {len(validation_set)} papers for validation to {validation_file}\")\n",
    "print(\"\\nDistribution by similarity bin:\")\n",
    "print(validation_set['similarity_bin'].value_counts().sort_index())\n",
    "\n",
    "# ===== Save file_a_sorted for later use =====\n",
    "file_a_sorted_file = os.path.join(OUTPUT_FOLDER, 'file_a_with_scores.pkl')\n",
    "file_a_sorted.to_pickle(file_a_sorted_file)\n",
    "print(f\"\\n‚úì Saved scored dataset to {file_a_sorted_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"1. Open {validation_file} in Excel\")\n",
    "print(\"2. Review papers and mark 'Y' for implementation science, 'N' for not\")\n",
    "print(\"3. Save as 'validation_sample_COMPLETED.xlsx' in the same folder\")\n",
    "print(\"4. Run the threshold determination code\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cece5b-a63c-4154-972f-ac02d35e73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Find validation threshold and apply labels\n",
    "# ========================================\n",
    "\n",
    "# ========================================\n",
    "# CONFIGURATION\n",
    "# ========================================\n",
    "OUTPUT_FOLDER = 'hiv imp proj validation'\n",
    "FILE_A_FOLDER = 'hiv not imp'\n",
    "# ========================================\n",
    "\n",
    "# Step 1: Read your reviews and find threshold\n",
    "print(\"Reading validation results...\")\n",
    "validation_file = os.path.join(OUTPUT_FOLDER, 'validation_sample_COMPLETED.xlsx')\n",
    "reviewed = pd.read_excel(validation_file)\n",
    "implementation_papers = reviewed[reviewed['YOUR_REVIEW'].str.upper() == 'Y']\n",
    "\n",
    "# Find optimal threshold\n",
    "threshold = implementation_papers['implementation_similarity'].min()\n",
    "print(f\"Suggested threshold: {threshold:.3f}\")\n",
    "print(f\"Papers marked as implementation in validation: {len(implementation_papers)}\")\n",
    "\n",
    "# Step 2: Load file_a_sorted\n",
    "file_a_sorted_file = os.path.join(OUTPUT_FOLDER, 'file_a_with_scores.pkl')\n",
    "print(f\"\\nLoading scored dataset from {file_a_sorted_file}...\")\n",
    "file_a_sorted = pd.read_pickle(file_a_sorted_file)\n",
    "\n",
    "# Step 3: Load the full dataset with location classification\n",
    "input_file = os.path.join(FILE_A_FOLDER, 'hiv_us_2000_2025_with_location_classification.csv')\n",
    "print(f\"\\nLoading full dataset: {input_file}\")\n",
    "full_dataset = pd.read_csv(input_file)\n",
    "print(f\"Total records in full dataset: {len(full_dataset):,}\")\n",
    "\n",
    "# Step 4: Merge similarity scores from file_a_sorted\n",
    "print(\"\\nMerging similarity scores...\")\n",
    "similarity_scores = file_a_sorted[['PMID', 'implementation_similarity']].copy()\n",
    "\n",
    "# Merge on PMID\n",
    "full_dataset = full_dataset.merge(similarity_scores, on='PMID', how='left')\n",
    "\n",
    "# Step 5: Add flag column\n",
    "full_dataset['likely_implementation'] = full_dataset['implementation_similarity'] >= threshold\n",
    "\n",
    "# Count how many are flagged\n",
    "flagged_count = full_dataset['likely_implementation'].sum()\n",
    "print(f\"\\nPapers flagged as likely implementation: {flagged_count:,}\")\n",
    "print(f\"Percentage of dataset: {flagged_count/len(full_dataset)*100:.2f}%\")\n",
    "\n",
    "# Step 6: Show distribution by similarity score\n",
    "print(\"\\nDistribution of similarity scores:\")\n",
    "print(full_dataset['implementation_similarity'].describe())\n",
    "\n",
    "print(f\"\\nBreakdown by flag:\")\n",
    "print(full_dataset['likely_implementation'].value_counts())\n",
    "\n",
    "# Step 7: Save updated dataset to output folder\n",
    "output_file = os.path.join(OUTPUT_FOLDER, 'hiv_us_2000_2025_with_location_and_impl_score.csv')\n",
    "full_dataset.to_csv(output_file, index=False)\n",
    "print(f\"\\n‚úì Saved updated dataset to: {output_file}\")\n",
    "\n",
    "# Step 8: Export just the likely implementation papers\n",
    "likely_impl_papers = full_dataset[full_dataset['likely_implementation'] == True].copy()\n",
    "likely_impl_output = os.path.join(OUTPUT_FOLDER, 'likely_implementation_papers.csv')\n",
    "likely_impl_papers.to_csv(likely_impl_output, index=False)\n",
    "print(f\"‚úì Saved {len(likely_impl_papers):,} likely implementation papers to: {likely_impl_output}\")\n",
    "\n",
    "# Step 9: Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Threshold used: {threshold:.3f}\")\n",
    "print(f\"Total papers in dataset: {len(full_dataset):,}\")\n",
    "print(f\"Papers above threshold: {flagged_count:,} ({flagged_count/len(full_dataset)*100:.2f}%)\")\n",
    "print(f\"\\nLocation breakdown of likely implementation papers:\")\n",
    "if 'Location_Detail' in likely_impl_papers.columns:\n",
    "    print(likely_impl_papers['Location_Detail'].value_counts())\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {OUTPUT_FOLDER}/\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481d271a-eac2-4219-8fbc-515949cf21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Filter final dataset as needed\n",
    "# ========================================\n",
    "\n",
    "# ========================================\n",
    "# CONFIGURATION\n",
    "# ========================================\n",
    "INPUT_FOLDER = 'hiv not imp'\n",
    "INPUT_FILE = 'hiv_us_2000_2025_with_location_and_impl_score.csv'\n",
    "OUTPUT_FILE = 'filtered_2015-2025_with_DOI_likely_impl_US.csv'\n",
    "# ========================================\n",
    "\n",
    "# Load the full dataset\n",
    "input_path = os.path.join(INPUT_FOLDER, INPUT_FILE)\n",
    "print(f\"Loading {input_path}...\")\n",
    "df = pd.read_csv(input_path)\n",
    "print(f\"Total records: {len(df):,}\")\n",
    "\n",
    "# Show what columns are available\n",
    "print(\"\\nAvailable columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Apply ALL your filters\n",
    "print(\"\\nApplying filters...\")\n",
    "\n",
    "# Filter 1: date_year between 2015-2025\n",
    "df_filtered = df[(df['date_year'] >= 2015) & (df['date_year'] <= 2025)].copy()\n",
    "print(f\"After date filter (2015-2025): {len(df_filtered):,}\")\n",
    "\n",
    "# Filter 2: Only rows with DOIs (not null/empty)\n",
    "df_filtered = df_filtered[df_filtered['DOI'].notna()].copy()\n",
    "df_filtered = df_filtered[df_filtered['DOI'].str.strip() != ''].copy()\n",
    "print(f\"After DOI filter (has DOI): {len(df_filtered):,}\")\n",
    "\n",
    "# Filter 3: likely_implementation = True\n",
    "df_filtered = df_filtered[df_filtered['likely_implementation'] == True].copy()\n",
    "print(f\"After likely_implementation filter (True): {len(df_filtered):,}\")\n",
    "\n",
    "# Filter 4: Location_Detail NOT \"Non-US only\"\n",
    "df_filtered = df_filtered[df_filtered['Location_Detail'] != 'Non-US only'].copy()\n",
    "print(f\"After location filter (NOT Non-US only): {len(df_filtered):,}\")\n",
    "\n",
    "# Show what's included in Location_Detail\n",
    "print(\"\\nLocation_Detail breakdown in filtered data:\")\n",
    "print(df_filtered['Location_Detail'].value_counts())\n",
    "\n",
    "# Save filtered version\n",
    "output_path = os.path.join(INPUT_FOLDER, OUTPUT_FILE)\n",
    "df_filtered.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úì SAVED: {output_path}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Final filtered records: {len(df_filtered):,}\")\n",
    "print(f\"Reduction: {len(df):,} ‚Üí {len(df_filtered):,} ({len(df_filtered)/len(df)*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
